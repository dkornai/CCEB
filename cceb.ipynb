{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33e716f",
   "metadata": {},
   "source": [
    "# Compositional Contextual Expert Bandits [CCEB]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804603e1",
   "metadata": {},
   "source": [
    "## Part 1: Defining The Inference System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b30700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set so that each particle will only use one thread\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "# set torch print options, including line width\n",
    "torch.set_printoptions(precision=4, sci_mode=False, linewidth=250)\n",
    "\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e0d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/venvs/generic312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from conjugates import ConjugateCategorical, ConjugateGaussian\n",
    "from crp import CRP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b07bc",
   "metadata": {},
   "source": [
    "#### Helper functions to simplify syntax of sampling categorical distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219285dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical2D():\n",
    "    def __init__(self, probs: torch.Tensor):\n",
    "        assert probs.dim() == 2, \"probs must be a 2D tensor\"\n",
    "        self.dim1, self.dim2 = probs.shape\n",
    "        #assert torch.sum(probs) == 1.0, \"probs must sum to 1\"\n",
    "        self.probs2d = probs  \n",
    "        self.probs = probs.flatten()\n",
    "\n",
    "    def sample(self):\n",
    "        idx = D.Categorical(probs=self.probs).sample().item()\n",
    "        i = idx // self.dim2\n",
    "        j = idx % self.dim2\n",
    "        \n",
    "        return i, j\n",
    "    \n",
    "def cat_sample(probs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Sample from a categorical distribution defined by the given probabilities.\n",
    "    probs: 1D tensor of probabilities\n",
    "    returns: sampled index\n",
    "    \"\"\"\n",
    "    normalized_probs = probs / torch.sum(probs)\n",
    "    return D.Categorical(probs=normalized_probs).sample().item()\n",
    "\n",
    "def cat2D_sample(probs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Sample from a 2D categorical distribution defined by the given probabilities.\n",
    "    probs: 2D tensor of probabilities\n",
    "    returns: sampled indices (i, j)\n",
    "    \"\"\"\n",
    "    cat2d = Categorical2D(probs)\n",
    "    return cat2d.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a44d1",
   "metadata": {},
   "source": [
    "### Main Particle Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee988c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle():\n",
    "    \"\"\"\n",
    "    Main class implementing a single particle in the CCEB model\n",
    "    \"\"\"\n",
    "    def __init__(self, hyp_gamma, hyp_alpha_o=1.0, hyp_alpha_r=1.0):\n",
    "        \n",
    "        self.hyp_gamma = hyp_gamma\n",
    "        self.hyp_alpha_o = hyp_alpha_o\n",
    "        self.hyp_alpha_r = hyp_alpha_r\n",
    "\n",
    "        self.state_space = [0, 1]\n",
    "        self.state_probs = torch.tensor([0.5, 0.5])\n",
    "        self.n_states = len(self.state_space)\n",
    "\n",
    "        self.context_o_space = []\n",
    "        self.context_r_space = []\n",
    "\n",
    "        self.observation_models = [[] for _ in self.state_space]\n",
    "        self.reward_models = [[] for _ in self.state_space]\n",
    "\n",
    "        self.context_o_CRP = CRP(hyp_alpha=self.hyp_alpha_o)\n",
    "        self.context_r_CRP = CRP(hyp_alpha=self.hyp_alpha_r)\n",
    "\n",
    "        self.prev_o_context = 0\n",
    "        self.prev_r_context = 0\n",
    "\n",
    "        self.novel_obs_model = ConjugateGaussian(**conjugate_gaussian_standard)\n",
    "        self.novel_reward_model = ConjugateCategorical(alpha0=alpha_0_standard)\n",
    "\n",
    "    @property\n",
    "    def n_contexts_o(self):\n",
    "        return len(self.context_o_space)\n",
    "\n",
    "    @property\n",
    "    def n_contexts_r(self):\n",
    "        return len(self.context_r_space)\n",
    "\n",
    "    def observation_likelihood(self, o_t):\n",
    "        \"\"\"\n",
    "        P(o_t | s_t=i, c_o_t=j)\n",
    "\n",
    "        What is the likelihood of the observation o_t under each state i and observation context j?\n",
    "        \"\"\"\n",
    "        likelihoods = torch.zeros(self.n_states, (self.n_contexts_o + 1))\n",
    "\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_contexts_o):\n",
    "                \"\"\"\n",
    "                P(o_t | s_t=i, c_o_t=j) = ∫ P(o_t | Ω_i,j) P(Ω_i,j| ... ) dΩ_i,j            Multivariate Student-t\n",
    "                \"\"\"\n",
    "                likelihoods[i, j] = self.observation_models[i][j].predictive_likelihood(o_t)\n",
    "            \"\"\"\n",
    "            P(o_t | s_t=i, c_o_t=new) = ∫ P(o_t | Ω_i,new) P(Ω_i,new| ... ) dΩ_i,new        Multivariate Student-t\n",
    "            \"\"\"\n",
    "            likelihoods[i, -1] = self.novel_obs_model.predictive_likelihood(o_t)\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def marginal_evidence(self, likelihoods):\n",
    "        \"\"\"\n",
    "        P(o_t |c_o_{t-1}, j_t=i) = Σ_s_t Σ_c_o_t P(o_t | S_t, C_o_t) P(S_t) P(C_o_t |c_o_{t-1}, j_t=i)\n",
    "\n",
    "        What is the marginal likelihood of the observation o_t under each value of the jump variable j_t?\n",
    "\n",
    "            This is used to sample the jump variable j_t to make proposals more efficient.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        P(o_t |c_o_{t-1}, j_t=0) = Σ_s_t P(o_t | S_t, c_o_{t-1}) P(S_t)\n",
    "        \"\"\"\n",
    "        # Special case for first time step, a jump must be made, since there is no previous context\n",
    "        if self.n_contexts_o == 0: \n",
    "            lj0 = 0.0\n",
    "        # Otherwise, marginal for not jumping is the likelihood under the previous context, weighted by state probabilities\n",
    "        else:\n",
    "            lj0 = torch.zeros(self.n_states)\n",
    "            for i in range(self.n_states):\n",
    "                lj0[i] = self.state_probs[i] * likelihoods[i, self.prev_o_context]\n",
    "            lj0 = torch.sum(lj0)\n",
    "\n",
    "        \"\"\"\n",
    "        P(o_t |c_o_{t-1}, j_t=1) = Σ_s_t Σ_c_o_t P(o_t | S_t, C_o_t) P(S_t) P(C_o_t | θ^o)\n",
    "        \"\"\"\n",
    "        lj1 = torch.zeros(self.n_states)\n",
    "        for i in range(self.n_states):\n",
    "            lj1[i] = self.state_probs[i] * torch.sum(likelihoods[i,:]*self.context_o_CRP.probabilities)\n",
    "        lj1 = torch.sum(lj1)\n",
    "\n",
    "        return lj0, lj1\n",
    "    \n",
    "    def sample_jump(self, likelihoods):\n",
    "        \"\"\"\n",
    "        j_t ~ P(J_t | o_t, ...) ∝ P(o_t | J_t, ...) P(J_t)\n",
    "\n",
    "        Sample the jump variable j_t. 0 = no jump, 1 = jump\n",
    "        \"\"\"\n",
    "\n",
    "        lj0, lj1 = self.marginal_evidence(likelihoods)\n",
    "        j_probs = torch.tensor([lj0*(1-self.hyp_gamma), lj1*self.hyp_gamma])\n",
    "\n",
    "        return cat_sample(j_probs)\n",
    "\n",
    "    def sample_state_context_o(self, likelihoods, j_t):\n",
    "        \"\"\"\n",
    "        s_t, c_o_t ~ P(S_t, C_o_t | o_t, j_t, c_o_{t-1} ...) ∝ P(o_t | S_t, C_o_t) P(S_t) P(C_o_t | j_t, c_o_{t-1})\n",
    "\n",
    "        Sample the latent state \n",
    "        \"\"\"\n",
    "        if j_t == 0:\n",
    "            \"\"\"\n",
    "            s_t, c_o_t ~ P(S_t, C_o_t | o_t, j_t=0, ...) ∝ P(o_t | S_t, C_o_t) P(s_t) δ (C_o_t, c_o_{t-1})\n",
    "            \"\"\"\n",
    "            s_t_probs = torch.zeros(self.n_states)\n",
    "            for i in range(self.n_states):\n",
    "                s_t_probs[i] = self.state_probs[i] * likelihoods[i, self.prev_o_context]\n",
    "\n",
    "            s_t = cat_sample(s_t_probs)    \n",
    "            c_o_t = self.prev_o_context        \n",
    "\n",
    "        elif j_t == 1:\n",
    "            \"\"\"\n",
    "            s_t, c_o_t ~ P(S_t, C_o_t | o_t, j_t=1, ...) ∝ P(o_t | S_t, C_o_t) P(S_t) P(C_o_t | θ^o) \n",
    "            \"\"\"\n",
    "            joint_probs = torch.zeros((self.n_states, self.n_contexts_o + 1))\n",
    "            for i in range(self.n_states):\n",
    "                # Existing contexts\n",
    "                for j in range(self.n_contexts_o):\n",
    "                    joint_probs[i, j] = self.state_probs[i] * likelihoods[i, j] * self.context_o_CRP.probabilities[j]\n",
    "                # New context\n",
    "                joint_probs[i, -1] = self.state_probs[i] * likelihoods[i, -1] * self.context_o_CRP.probabilities[-1]\n",
    "\n",
    "            s_t, c_o_t = cat2D_sample(joint_probs)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"j_t must be 0 or 1\")\n",
    "\n",
    "        return s_t, c_o_t\n",
    "    \n",
    "    def sample_context_r(self, j_t):\n",
    "        \"\"\"\n",
    "        c_r_t ~ P(c_r_t | j_t, ...) = P(c_r_t | θ^r) if j_t=1 else δ(c_r_t, c_r_{t-1})\n",
    "\n",
    "        Sample the reward context given the jump variable\n",
    "        \"\"\"\n",
    "\n",
    "        if j_t == 0:\n",
    "            c_r_t = self.prev_r_context\n",
    "\n",
    "        else: # if j == 1\n",
    "            c_r_t = self.context_r_CRP.sample()\n",
    "\n",
    "        return c_r_t\n",
    "\n",
    "    def sample_latent_states(self, o_t):\n",
    "        \"\"\"\n",
    "        s_t, c_o_t, c_r_t, j_t ~ P(S_t, C_o_t, C_r_t, J_t | o_t, ...)\n",
    "\n",
    "        Sample the latent state, observation context, reward context, and jump variable given the new observation o_t\n",
    "        \"\"\"\n",
    "        likelihoods = self.observation_likelihood(o_t)\n",
    "\n",
    "        \"\"\"\n",
    "        P(J_t | o_t, ...) ∝ P(o_t | J_t, ...) P(J_t)\n",
    "        \"\"\"\n",
    "        j_t = self.sample_jump(likelihoods)\n",
    "        \"\"\"\n",
    "        P(S_t, S_o_t | o_t, j_t, ...) ∝ P(o_t | S_t, C_o_t) P(S_t) P(C_o_t | j_t, c_o_{t-1})\n",
    "        \"\"\"\n",
    "        s_t, c_o_t = self.sample_state_context_o(likelihoods, j_t)\n",
    "        # Handle new observation context if needed\n",
    "        if c_o_t == self.n_contexts_o: \n",
    "            self.context_o_space.append(self.n_contexts_o)\n",
    "            for i, state in enumerate(self.state_space):\n",
    "                self.observation_models[i].append(ConjugateGaussian(**conjugate_gaussian_standard))\n",
    "        \"\"\"\n",
    "        P(C_r_t | j_t, ...) = P(C_r_t | θ^r) if j_t=1 else δ(c_r_t, c_r_{t-1})\n",
    "        \"\"\"\n",
    "        c_r_t = self.sample_context_r(j_t)\n",
    "        # Handle new reward context if needed\n",
    "        if c_r_t == len(self.context_r_space): \n",
    "            self.context_r_space.append(len(self.context_r_space))\n",
    "            for i, state in enumerate(self.state_space):\n",
    "                self.reward_models[i].append(ConjugateCategorical(alpha0=alpha_0_standard))\n",
    "\n",
    "        return s_t, c_o_t, c_r_t, j_t\n",
    "    \n",
    "    \n",
    "    def update_o_params(self, s_t, c_o_t, o_t, j_t):\n",
    "        \"\"\"\n",
    "        Update the observation models and the observation context CRP\n",
    "        \"\"\"       \n",
    "        \n",
    "        \"\"\"\n",
    "        Update the observation model parameter distribution Ω_i,j based on the new observation o_t (handled internally by the conjugate model class)\n",
    "        \"\"\"\n",
    "        self.observation_models[s_t][c_o_t].update(o_t)\n",
    "        \n",
    "        \"\"\"\n",
    "        Update the CRP counts θ^o if a jump was made\n",
    "        \"\"\"\n",
    "        if j_t == 1.0:\n",
    "            self.context_o_CRP.update_single(c_o_t)\n",
    "            \n",
    "\n",
    "    def sample_action(self, s_t, c_r_t):\n",
    "        \"\"\"\n",
    "        Thompson sampling action selection based on the reward model for the current state and reward context\n",
    "        \"\"\"\n",
    "\n",
    "        predicted_rewards = self.reward_models[s_t][c_r_t].sample_posterior_distribution()\n",
    "    \n",
    "        greedy_a_t = torch.argmax(predicted_rewards).item()\n",
    "\n",
    "        # make epsilon-greedy here if desired\n",
    "        if torch.rand(1).item() < 0.01:\n",
    "            # choose random action from [0, 1, 2, 3]\n",
    "            a_t = torch.randint(0, 4, (1,)).item()\n",
    "        else:\n",
    "            a_t = greedy_a_t\n",
    "\n",
    "        return a_t\n",
    "\n",
    "\n",
    "    def reward_likelihood(self, r_t, s_t, a_t, c_r_t):\n",
    "        \"\"\"\n",
    "        P(r_t={0,1} | s_t=i, c_r_t=k, a_t=a, ϒ) \n",
    "\n",
    "        Compute the predictive likelihood of a reward r_t given the state s_t, reward context c_r_t, action a_t, and the reward model parameter distribution ϒ_i,k\n",
    "        \"\"\"\n",
    "        if r_t == 1.0:\n",
    "            return self.reward_models[s_t][c_r_t].predictive_likelihood(a_t)\n",
    "        else:\n",
    "            return 1.0 - self.reward_models[s_t][c_r_t].predictive_likelihood(a_t)\n",
    "        \n",
    "\n",
    "    def update_r_params(self, c_r_t, s_t, a_t, r_t, j_t):\n",
    "        \"\"\"\n",
    "        Update the reward models and the reward context CRP\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Update the reward model parameter distribution ϒ_i,j based on the action taken and reward received\n",
    "        \"\"\"\n",
    "        # If the action was optimal, update the model with the actual action taken\n",
    "        if r_t == 1.0:\n",
    "            self.reward_models[s_t][c_r_t].update(a_t)\n",
    "        # If the action was not optimal, sample an alternative action and update the model with that\n",
    "        else:\n",
    "            probs = self.reward_models[s_t][c_r_t].posterior_params()['alpha_n']\n",
    "\n",
    "            probs[a_t] = 0.0\n",
    "            probs = probs / probs.sum()\n",
    "            alt_a_t = D.Categorical(probs=probs).sample().item()\n",
    "\n",
    "            self.reward_models[s_t][c_r_t].update(alt_a_t)\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Update the CRP counts θ^r if a jump was made\n",
    "        \"\"\"\n",
    "        if j_t == 1.0:\n",
    "            self.context_r_CRP.update_single(c_r_t)\n",
    "\n",
    "    def predictive_evidence(self, o_t):\n",
    "        \"\"\"\n",
    "        P(o_t | ...) =\n",
    "\n",
    "        Compute the marginal likelihood of the observation o_t under the particle's model\n",
    "        \"\"\"\n",
    "        likelihoods = self.observation_likelihood(o_t)\n",
    "        lj0, lj1 = self.marginal_evidence(likelihoods)\n",
    "        p_evidence = (1 - self.hyp_gamma) * lj0 + self.hyp_gamma * lj1\n",
    "        \n",
    "        return p_evidence\n",
    "\n",
    "\n",
    "# Standard prior parameters for conjugate models\n",
    "conjugate_gaussian_standard = {\n",
    "    'mu0': torch.tensor([0.0, 0.0, 0.0]),\n",
    "    'kappa0': 0.1,\n",
    "    'nu0': 4.0,\n",
    "    'Lambda0': torch.eye(3) * 1.0 \n",
    "}\n",
    "alpha_0_standard = torch.tensor([0.1, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f95eb",
   "metadata": {},
   "source": [
    "### Ensemble Class implementing filtering steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd05eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self, N, hyp_gamma=0.05, hyp_alpha_o=1.0, hyp_alpha_r=1.0):\n",
    "        self.N = N\n",
    "        self.particles = [Particle(hyp_gamma, hyp_alpha_o, hyp_alpha_r) for _ in range(N)]\n",
    "        self.weights = torch.ones(N) / N\n",
    "\n",
    "        self.vec_s_t    = torch.zeros(N, dtype=torch.long)\n",
    "        self.vec_c_o_t  = torch.zeros(N, dtype=torch.long)\n",
    "        self.vec_c_r_t  = torch.zeros(N, dtype=torch.long)\n",
    "        self.vec_j_t    = torch.zeros(N, dtype=torch.long)\n",
    "\n",
    "    def resample_particles(self):\n",
    "        \"\"\"\n",
    "        Resample particles according to their weights\n",
    "        \"\"\"\n",
    "        # Normalise weights\n",
    "        self.weights /= torch.sum(self.weights)\n",
    "        \n",
    "        # Resample particles according to weights\n",
    "        indices = D.Categorical(probs=self.weights).sample((self.N,))\n",
    "        self.particles = [deepcopy(self.particles[i]) for i in indices]\n",
    "        \n",
    "        # Reset weights to uniform after resampling\n",
    "        self.weights = torch.ones(self.N) / self.N\n",
    "\n",
    "    def before_action(self, o_t):\n",
    "        \"\"\"\n",
    "        Before an action is taken, process the new observation o_t\n",
    "        \"\"\"\n",
    "        print('> observation:', o_t)\n",
    "\n",
    "        # 1) Resample particles based on predictive evidence (auxiliary particle filter)\n",
    "        for i, p in enumerate(self.particles):\n",
    "            self.weights[i] = p.predictive_evidence(o_t)\n",
    "\n",
    "        # 2) Sample latent variables and update observation models\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            s_t, c_o_t, c_r_t, j_t = particle.sample_latent_states(o_t)\n",
    "\n",
    "            self.vec_s_t[i]     = s_t\n",
    "            self.vec_c_o_t[i]   = c_o_t\n",
    "            self.vec_j_t[i]     = j_t\n",
    "            self.vec_c_r_t[i]   = c_r_t\n",
    "\n",
    "            particle.update_o_params(s_t, c_o_t, o_t, j_t)\n",
    "           \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action\n",
    "        \"\"\"\n",
    "        actions = torch.zeros(self.N, dtype=torch.long)\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            actions[i] = particle.sample_action(self.vec_s_t[i], self.vec_c_r_t[i])\n",
    "\n",
    "        selected_action = actions[D.Categorical(probs=self.weights).sample().item()]\n",
    "\n",
    "        print(actions)\n",
    "\n",
    "        return selected_action, actions\n",
    "\n",
    "    def after_action(self, a_t, r_t):\n",
    "        \"\"\"\n",
    "        Update the model after an action a_t is taken and reward r_t is received\n",
    "        \"\"\"\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            self.weights[i] = particle.reward_likelihood(r_t, self.vec_s_t[i], a_t, self.vec_c_r_t[i])\n",
    "            particle.update_r_params(self.vec_c_r_t[i], self.vec_s_t[i], a_t, r_t, self.vec_j_t[i])\n",
    "\n",
    "            particle.prev_o_context = self.vec_c_o_t[i]\n",
    "            particle.prev_r_context = self.vec_c_r_t[i]\n",
    "\n",
    "        self.resample_particles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d508f",
   "metadata": {},
   "source": [
    "## Part 2: Defining the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a23a4",
   "metadata": {},
   "source": [
    "### Define the various contexts that can be encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564da1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE = 0.02\n",
    "\n",
    "dist_c_0_s_0 = D.MultivariateNormal(loc=torch.tensor([1.0, 0.0, 0.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "dist_c_0_s_1 = D.MultivariateNormal(loc=torch.tensor([-1.0, 0.0, 0.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "dist_c_1_s_0 = D.MultivariateNormal(loc=torch.tensor([0.0, 1.0, 0.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "dist_c_1_s_1 = D.MultivariateNormal(loc=torch.tensor([0.0, -1.0, 0.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "dist_c_2_s_0 = D.MultivariateNormal(loc=torch.tensor([0.0, 0.0, 1.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "dist_c_2_s_1 = D.MultivariateNormal(loc=torch.tensor([0.0, 0.0, -1.0]), covariance_matrix=torch.eye(3)*NOISE)\n",
    "\n",
    "def generate_observation(true_c_o, true_s):\n",
    "    \"\"\"\n",
    "    Three possible observation contexts\n",
    "    \"\"\"\n",
    "    if true_c_o == 0:\n",
    "        if true_s == 0:\n",
    "            return dist_c_0_s_0.sample()\n",
    "        else:\n",
    "            return dist_c_0_s_1.sample()\n",
    "    elif true_c_o == 1:\n",
    "        if true_s == 0:\n",
    "            return dist_c_1_s_0.sample()\n",
    "        else:\n",
    "            return dist_c_1_s_1.sample()\n",
    "    elif true_c_o == 2:\n",
    "        if true_s == 0:\n",
    "            return dist_c_2_s_0.sample()\n",
    "        else:\n",
    "            return dist_c_2_s_1.sample()\n",
    "    else:\n",
    "        raise ValueError(\"true_c must be 0, 1, or 2\")\n",
    "\n",
    "\n",
    "def generate_reward(true_s, a_t, true_c_r):\n",
    "    \"\"\"\n",
    "    Two possible reward contexts\n",
    "    \"\"\"\n",
    "    \n",
    "    if true_c_r == 0:\n",
    "        if true_s == 0:\n",
    "            return 1.0 if a_t == 0 else 0.0\n",
    "        else:\n",
    "            return 1.0 if a_t == 1 else 0.0\n",
    "    elif true_c_r == 1:\n",
    "        if true_s == 0:\n",
    "            return 1.0 if a_t == 2 else 0.0\n",
    "        else:\n",
    "            return 1.0 if a_t == 3 else 0.0\n",
    "    else:\n",
    "        raise ValueError(\"true_s must be 0 or 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d21f3b",
   "metadata": {},
   "source": [
    "#### Definte a series of trials in the same context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08be9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history buffers\n",
    "context_o_hist = []\n",
    "context_r_hist = []\n",
    "actions_hist = []\n",
    "jumps_hist = []\n",
    "true_rewarding_actions = []\n",
    "\n",
    "\n",
    "def batched_trials(ensemble: Ensemble, true_c_o_t, true_c_r_t, n_steps=50):\n",
    "    \"\"\"\n",
    "    Run a batch of trials with the given context combination.\n",
    "    \"\"\"\n",
    "    for i in range(n_steps):\n",
    "        # Sample state and observation\n",
    "        true_s_t = cat_sample(torch.tensor([0.5, 0.5]))\n",
    "        o_t = generate_observation(true_c_o_t, true_s_t)\n",
    "        \n",
    "        # Process observation\n",
    "        ensemble.before_action(o_t)    \n",
    "        \n",
    "        # Select action\n",
    "        a_t, actions = ensemble.select_action()\n",
    "        \n",
    "        # Generate reward\n",
    "        r_t = generate_reward(true_s_t, a_t, true_c_r_t)\n",
    "        \n",
    "        # Update model\n",
    "        ensemble.after_action(a_t, r_t)\n",
    "        \n",
    "        # Record keeping\n",
    "        context_o_hist.append(ensemble.vec_c_o_t.clone())\n",
    "        jumps_hist.append(ensemble.vec_j_t.clone())\n",
    "        actions_hist.append(actions.clone())\n",
    "        if true_c_r_t == 0:\n",
    "            true_rewarding_actions.append(0 if true_s_t == 0 else 1)\n",
    "        elif true_c_r_t == 1:\n",
    "            true_rewarding_actions.append(2 if true_s_t == 0 else 3)\n",
    "        context_r_hist.append(ensemble.vec_c_r_t.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a856487",
   "metadata": {},
   "source": [
    "## Part 3: Simulation and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740e203",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472df8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> observation: tensor([ 0.9340,  0.0567, -0.3054])\n",
      "tensor([3, 0, 2, 2, 1, 1, 0, 0, 1, 0, 3, 3, 3, 1, 2, 2, 1, 3, 2, 2, 1, 3, 3, 0, 0, 1, 2, 2, 2, 3, 2, 3, 0, 1, 3, 2, 2, 1, 3, 1, 2, 0, 3, 0, 2, 3, 0, 3, 1, 1, 3, 2, 1, 2, 1, 2, 1, 1, 3, 0, 3, 2, 0, 2, 3, 1, 1, 3, 3, 0, 3, 2, 3, 0, 2, 1, 3, 1, 1, 1, 3,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 3, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 3, 1, 2, 3, 3, 3, 0, 0, 2, 3, 0, 2, 2, 3, 0, 0, 1, 1, 1, 3, 1, 0, 1, 3, 1, 3, 2, 1, 3, 1, 0, 0, 3, 1, 1, 2, 2, 1, 0, 0, 2, 1, 3, 1, 3, 0, 3, 2, 3, 3, 1, 3, 2, 3, 0, 3, 0,\n",
      "        1, 2, 3, 3, 3, 0, 1, 3, 3, 1, 0, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 3, 0, 0, 0, 0, 0, 1, 3, 0, 2, 1, 1, 3, 2, 2, 3, 0, 1, 0, 3, 1, 3, 2, 3, 0, 3, 1, 3, 2, 3, 1, 1, 0, 3, 1, 1, 1, 2, 0, 1, 1, 3, 1, 3, 1, 1, 3, 0, 3, 2, 1, 2, 0, 1, 1, 0, 2, 2, 2, 3,\n",
      "        1, 2, 1, 2, 1, 2, 3])\n",
      "> observation: tensor([ 1.0683, -0.0626,  0.0601])\n",
      "tensor([0, 0, 3, 3, 0, 2, 3, 2, 3, 0, 0, 1, 2, 3, 1, 3, 3, 3, 1, 0, 0, 3, 3, 0, 3, 2, 0, 3, 1, 1, 0, 0, 2, 1, 0, 3, 1, 3, 3, 3, 1, 3, 2, 0, 0, 3, 0, 3, 1, 3, 2, 3, 3, 3, 3, 3, 0, 1, 3, 2, 2, 2, 1, 3, 0, 0, 1, 2, 3, 2, 1, 0, 0, 3, 0, 1, 0, 3, 3, 0, 1,\n",
      "        0, 1, 1, 1, 0, 3, 3, 2, 0, 0, 3, 2, 1, 3, 0, 1, 3, 3, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 0, 3, 3, 0, 1, 3, 1, 1, 3, 1, 1, 3, 0, 3, 3, 3, 0, 1, 0, 0, 3, 1, 1, 0, 1, 2, 0, 0, 3, 0, 0, 3, 3, 3, 2, 1, 2, 0, 0, 1, 1, 1, 1, 3, 1, 0, 1, 1, 3, 3, 3, 3,\n",
      "        0, 1, 3, 2, 2, 2, 3, 0, 3, 0, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 0, 0, 3, 1, 0, 0, 3, 1, 3, 0, 0, 0, 0, 3, 2, 0, 0, 3, 1, 3, 2, 3, 3, 0, 0, 1, 1, 0, 1, 2, 1, 0, 3, 0, 1, 3, 1, 0, 3, 0, 1, 1, 0, 0, 0, 1, 1, 1, 3, 3, 3, 0, 0, 1, 3, 3, 3, 3, 3, 1, 1,\n",
      "        1, 2, 3, 2, 2, 0, 0])\n",
      "> observation: tensor([-0.7682, -0.0756, -0.0915])\n",
      "tensor([2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 2, 0, 3, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0,\n",
      "        0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 0, 3, 0, 0, 2, 1, 0, 2, 3, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 3, 3, 1, 1, 0, 2, 3, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 1, 0, 2, 1, 0, 3, 1, 0, 2, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 3, 0, 0, 1, 0, 0, 0, 2, 0, 3, 0, 3, 0, 0, 3, 3, 3, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 3, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 2, 0, 0,\n",
      "        1, 2, 1, 3, 0, 0, 1])\n",
      "> observation: tensor([-0.6873, -0.0056, -0.1060])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2,\n",
      "        3, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 3, 0, 3, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 1, 3,\n",
      "        1, 1, 1, 0, 1, 0, 0])\n",
      "> observation: tensor([ 0.9823, -0.0265, -0.1185])\n",
      "tensor([1, 0, 0, 0, 0, 0, 3, 0, 2, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 3, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        3, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 3, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 0, 1, 0, 3, 0, 0, 1, 0, 1, 1, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 3, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 3, 3, 0, 0])\n",
      "> observation: tensor([ 0.9336, -0.0460, -0.0034])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 1, 3, 0, 1, 0, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 3, 3, 0, 0, 3, 3,\n",
      "        0, 1, 3, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 3, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 3, 3, 0, 3, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0])\n",
      "> observation: tensor([ 1.1683, -0.0475, -0.1689])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([1.0959, 0.1536, 0.0191])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.9973,  0.0497, -0.0199])\n",
      "tensor([1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 3, 1, 1])\n",
      "> observation: tensor([-1.2253, -0.1643,  0.1589])\n",
      "tensor([1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 1])\n",
      "> observation: tensor([-0.9233,  0.0446,  0.0718])\n",
      "tensor([1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.9291, -0.0638,  0.0861])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.0141, -0.2376,  0.1577])\n",
      "tensor([0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.8467, -0.0526, -0.1061])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.9866, -0.2825,  0.0945])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.9197, -0.1055, -0.3131])\n",
      "tensor([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.1234, -0.0723, -0.0260])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.0320, -0.2713,  0.1375])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.1543, -0.1240, -0.0732])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.8645, -0.0580,  0.1059])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.1224, -0.3103,  0.0812])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.7922,  0.0715,  0.1270])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.9187, -0.2703, -0.0589])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.3404, -0.1022,  0.3925])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.0045,  0.0777, -0.0154])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.9764, -0.1420,  0.0459])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.9573, -0.1219, -0.1413])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([0.9041, 0.0372, 0.0674])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.9868,  0.1452, -0.1985])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.8985, -0.1558, -0.1762])\n",
      "tensor([1, 1, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-1.2652, -0.1270, -0.2065])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.9156, -0.1027,  0.1135])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.9609, -0.1055, -0.1356])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.9369, -0.0606, -0.1645])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.1575,  0.1175, -0.0951])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.6153,  0.1734,  0.1622])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-1.1681, -0.0034,  0.0400])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.9977, -0.1171,  0.1728])\n",
      "tensor([0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.0473,  0.1626, -0.1028])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-1.1503,  0.1122, -0.0767])\n",
      "tensor([1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.0111, -0.0166, -0.0171])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 1.1931, -0.0645,  0.1014])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.0281,  0.0719, -0.2820])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.9953, -0.0277, -0.1308])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.9079, -0.0730, -0.1630])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.8082, -0.1578, -0.2365])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-1.0536,  0.1734,  0.0712])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.6496,  0.0729, -0.0705])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 1.0353, -0.0185, -0.0640])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.8046, -0.1935,  0.0871])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0739, -0.9271,  0.1194])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0])\n",
      "> observation: tensor([ 0.0232, -1.0921, -0.0780])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.2565, -0.8902,  0.2871])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0132, -0.9391, -0.1037])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        2, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1043,  0.9342, -0.0458])\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 3, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1])\n",
      "> observation: tensor([-0.1627,  1.3540, -0.0347])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1])\n",
      "> observation: tensor([-0.1836,  1.1183, -0.0130])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0])\n",
      "> observation: tensor([    -0.0006,     -0.9845,     -0.0753])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 3, 0, 1])\n",
      "> observation: tensor([ 0.1133, -1.0072, -0.0146])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.3407, -0.9016, -0.0357])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.0021, -0.9186, -0.0536])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1847, -1.0982, -0.0457])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.0997, -0.8718,  0.0698])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0255, -1.0015,  0.0275])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.2280,  0.6954,  0.2331])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.1582, -1.0836, -0.0184])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.2136,  1.2537,  0.1914])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0513, -0.9052, -0.2522])\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0752,  0.9375, -0.0964])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.0878, -1.0021, -0.0684])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([0.0653, 1.1336, 0.0434])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.1048, -1.1823, -0.0252])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0755, -0.8991,  0.1616])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0093, -1.2163,  0.0821])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.1542, -1.0785, -0.0357])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0299,  1.0054, -0.0285])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.1097,  0.8799,  0.1203])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.0108,  0.8402, -0.1697])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0161, -1.2161,  0.1931])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0509, -0.8235, -0.0092])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.0029,  0.8791,  0.2832])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.1294, -0.8691, -0.3644])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 3, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0203,  1.1982, -0.1543])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0516, -0.9499, -0.0034])\n",
      "tensor([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.1126,  1.0558, -0.1271])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.2612,  1.1717, -0.0328])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.2267, -0.8238, -0.2245])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1145,  1.3343,  0.0306])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.1830, -1.1406,  0.0133])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1020,  1.3027, -0.0273])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0615, -0.9077,  0.0485])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([0.1137, 0.8526, 0.0301])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.0694, -1.0175,  0.0436])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0684, -0.8537,  0.0215])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0302,  1.0948, -0.1569])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 2])\n",
      "> observation: tensor([-0.0840,  0.8486,  0.1882])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.1084, -1.0124,  0.2338])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([ 0.0716, -1.1473,  0.1666])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1787,  1.1016,  0.1182])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([-0.3932, -1.1109, -0.3040])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([-0.1995, -0.0732, -1.1329])\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 3, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0])\n",
      "> observation: tensor([ 0.1511,  0.1936, -1.0830])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "> observation: tensor([ 0.1695,  0.0528, -1.0601])\n",
      "tensor([1, 3, 1, 1, 1, 1, 1, 3, 1, 0, 1, 0, 3, 0, 1, 1, 1, 3, 3, 1, 3, 1, 1, 0, 1, 1, 0, 3, 3, 0, 1, 0, 3, 3, 1, 2, 1, 1, 3, 1, 3, 0, 1, 3, 0, 1, 1, 0, 2, 1, 3, 3, 1, 1, 3, 2, 0, 0, 1, 1, 0, 3, 1, 0, 1, 0, 1, 3, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 1, 3, 0,\n",
      "        0, 0, 0, 1, 1, 3, 0, 3, 0, 1, 0, 1, 3, 1, 0, 1, 3, 3, 1, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 3, 0, 3, 1, 1, 0, 1, 1, 1, 3, 0, 2, 0, 0, 0, 3, 0, 3, 1, 3, 0, 1, 1, 0, 3, 0, 1, 0, 1, 0, 1, 1, 1, 3, 0, 0, 0, 0, 1, 1, 3, 2, 1, 3, 1, 1,\n",
      "        3, 1, 1, 1, 1, 2, 3, 1, 1, 0, 1, 1, 0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 1, 3, 1, 1, 3, 0, 1, 3, 3, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 3, 1, 0, 1, 0, 1, 3, 1, 1, 1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 3, 0, 0, 1, 1, 1, 3, 1, 1, 1,\n",
      "        0, 0, 0, 3, 0, 0, 1])\n",
      "> observation: tensor([0.0751, 0.1535, 1.0806])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 3, 3, 1, 0, 1, 0, 0, 3, 0, 3, 2, 1, 1, 0, 3, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 2, 3, 3, 1, 1, 1, 1, 3, 0, 3, 1, 1, 0, 1, 1, 3, 2, 1, 0, 3, 0, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 3, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 1, 2, 1, 0, 1, 3,\n",
      "        1, 1, 3, 1, 0, 3, 1, 3, 1, 1, 1, 1, 3, 0, 2, 3, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 0, 1, 1, 3, 3, 1, 3, 1, 3, 3, 0, 0, 1, 3, 2, 3, 0, 1, 1, 3, 3, 1, 0, 1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1,\n",
      "        1, 3, 3, 3, 2, 0, 3, 3, 1, 0, 1, 3, 3, 3, 1, 0, 3, 3, 1, 3, 0, 3, 0, 3, 0, 3, 1, 0, 2, 1, 1, 0, 1, 3, 1, 1, 0, 0, 1, 3, 1, 1, 1, 0, 0, 0, 1, 0, 1, 3, 0, 0, 1, 0, 1, 1, 2, 2, 1, 0, 1, 2, 3, 3, 3, 3, 0, 0, 3, 1, 1, 0, 0, 1, 0, 1, 1, 1, 3, 1, 3,\n",
      "        2, 0, 1, 1, 0, 1, 1])\n",
      "> observation: tensor([-0.1431, -0.0557,  0.8666])\n",
      "tensor([3, 3, 0, 1, 3, 1, 2, 2, 2, 1, 1, 3, 2, 2, 2, 1, 3, 1, 0, 1, 1, 3, 2, 1, 3, 1, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 3, 0, 1, 3, 3, 3, 3, 2, 1, 1, 1, 1, 1, 3, 3, 1, 1, 0, 1, 1, 3, 1, 1, 1,\n",
      "        3, 3, 1, 3, 1, 2, 1, 3, 1, 1, 2, 2, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 3, 0, 3, 3, 0, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 3, 3, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0, 3, 1, 3, 1, 0, 1, 1, 0, 0, 1, 2, 3, 3, 1, 3, 3, 2, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1,\n",
      "        1, 3, 2, 0, 3, 1, 1, 3, 2, 3, 0, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 0, 2, 1, 3, 1, 1, 0, 3, 1, 2, 1, 1, 3, 3, 1, 1, 1, 3, 1, 0, 3, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 3,\n",
      "        0, 3, 1, 1, 3, 1, 1])\n",
      "> observation: tensor([0.0433, 0.0679, 1.0755])\n",
      "tensor([3, 3, 3, 3, 3, 3, 0, 3, 0, 0, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 2, 0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 0, 1, 3, 2, 3, 2, 2, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 0, 3, 0, 2, 2, 2, 3, 3, 3, 2, 0, 2, 3,\n",
      "        2, 2, 3, 3, 0, 3, 2, 3, 2, 0, 2, 3, 3, 3, 3, 2, 2, 1, 3, 3, 2, 1, 3, 3, 3, 3, 3, 1, 3, 0, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 2, 3, 0, 2, 3, 2, 2, 1, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 0, 0, 2, 2, 3, 3, 3, 3, 3, 1, 2, 3, 3, 2, 1,\n",
      "        3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 1, 3, 3, 3, 2, 1, 0, 2, 3, 3, 1, 2, 0, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 3, 3, 2, 0, 0, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 0, 1,\n",
      "        2, 2, 3, 3, 3, 2, 3])\n",
      "> observation: tensor([    -0.0006,      0.0090,      0.9100])\n",
      "tensor([3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 2, 3, 2, 0, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 1, 2, 2, 2, 3, 3, 2, 3, 0, 3, 3, 0, 3, 1, 3, 3, 3, 2, 3, 3, 1, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 0, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 0,\n",
      "        3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 2, 0, 3, 3, 2, 2, 2, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 0, 3, 3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 2, 3, 3, 3, 0, 0, 3, 3, 0, 3, 3, 3, 3, 2, 3,\n",
      "        3, 0, 3, 1, 3, 3, 3, 3, 0, 1, 3, 3, 2, 1, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 1, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 0, 1, 3, 2, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3,\n",
      "        3, 3, 3, 0, 2, 2, 3])\n",
      "> observation: tensor([-0.2821, -0.0097,  0.9148])\n",
      "tensor([1, 1, 2, 0, 1, 1, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 3, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 1, 0, 2, 2, 2, 1, 3, 2, 1, 2, 0, 2, 0, 2, 3, 3, 3, 0, 1, 2, 0, 3, 0, 1, 3, 2, 0, 3, 0, 2, 0, 2, 2, 1, 0, 1, 3, 0, 0, 2, 2, 3, 3, 0, 1, 2, 2, 2, 0, 2, 3,\n",
      "        3, 2, 2, 3, 0, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 2, 1, 2, 0, 0, 2, 2, 0, 2, 1, 3, 2, 0, 2, 3, 0, 0, 0, 3, 3, 1, 1, 2, 2, 3, 2, 0, 2, 2, 0, 3, 2, 2, 2, 0, 1, 0, 0, 2, 2, 3, 2, 1, 0, 2, 1, 0, 1, 2, 2, 0, 0, 0, 2, 0, 1, 2, 0, 3, 2, 2, 2, 2, 3, 1, 1,\n",
      "        3, 2, 3, 2, 2, 2, 2, 3, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 2, 3, 0, 3, 0, 1, 0, 1, 2, 3, 1, 1, 2, 2, 0, 0, 2, 0, 3, 2, 0, 3, 1, 1, 0, 3, 3, 2, 0, 0, 2, 0, 2, 0, 3, 0, 0, 0, 2, 2, 1, 2, 3, 3, 3, 3, 0, 0, 3, 0, 2, 3, 0, 0, 3, 3, 2, 0, 0, 0, 2, 3, 0,\n",
      "        3, 2, 0, 0, 0, 2, 2])\n",
      "> observation: tensor([-0.1272, -0.0499, -1.2168])\n",
      "tensor([3, 3, 2, 1, 3, 3, 3, 0, 1, 3, 3, 2, 3, 1, 1, 3, 3, 1, 1, 0, 2, 1, 3, 2, 0, 2, 1, 3, 3, 0, 3, 0, 3, 1, 1, 3, 0, 3, 3, 3, 3, 3, 3, 0, 3, 3, 0, 0, 1, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 3, 2, 1, 1, 1, 2, 1, 2, 3, 1, 3, 1, 1, 0, 3, 1, 2, 1, 2, 0, 3, 0,\n",
      "        0, 1, 3, 2, 3, 3, 3, 3, 0, 3, 1, 3, 2, 1, 1, 2, 0, 1, 2, 1, 3, 3, 3, 2, 1, 3, 2, 2, 2, 3, 3, 0, 1, 3, 0, 1, 1, 1, 1, 3, 3, 3, 1, 1, 0, 2, 3, 0, 3, 3, 2, 3, 1, 1, 3, 3, 0, 3, 3, 1, 2, 3, 2, 1, 3, 1, 2, 0, 3, 1, 3, 2, 1, 2, 1, 1, 1, 1, 3, 0, 0,\n",
      "        0, 1, 3, 3, 1, 1, 3, 1, 0, 1, 3, 0, 3, 0, 3, 3, 1, 3, 3, 2, 3, 1, 2, 1, 0, 1, 2, 3, 1, 1, 3, 3, 3, 3, 3, 3, 0, 2, 0, 1, 2, 2, 1, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 1, 0, 3, 2, 2, 0, 0, 1, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 0, 0, 1, 1, 3, 3, 1, 2, 1, 3,\n",
      "        3, 1, 3, 3, 3, 0, 3])\n",
      "> observation: tensor([-0.2837,  0.0952, -0.8248])\n",
      "tensor([1, 3, 0, 3, 3, 3, 1, 0, 3, 0, 1, 3, 3, 0, 0, 1, 3, 1, 0, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 0, 1, 0, 0, 0, 3, 0, 3, 3, 1, 3, 3, 1, 2, 3, 3, 1, 3, 1, 3, 0, 3, 1, 0, 3, 1, 3, 3, 3, 1, 1, 3, 0, 3, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 3, 1, 3, 0,\n",
      "        3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 2, 3, 1, 3, 3, 1, 1, 0, 0, 2, 3, 3, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 0, 1, 3, 0, 1, 1, 1, 1, 3, 1, 1, 3, 3, 0, 3, 0, 3, 3, 1, 1, 0, 3, 1, 3, 3, 1, 1, 3, 3, 0, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 1, 1, 2, 3, 1, 1, 1, 0, 3, 1, 1, 0, 1, 0, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 3, 2, 1, 1, 1, 3, 1, 0, 3, 3, 0, 0, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 3, 3, 1, 0, 1, 3, 3, 1, 1, 0, 1, 3, 3, 1, 0, 3, 3, 1, 0, 1, 1, 1, 3,\n",
      "        0, 1, 0, 3, 3, 3, 1])\n",
      "> observation: tensor([ 0.1638,  0.0420, -1.2213])\n",
      "tensor([1, 3, 3, 1, 3, 3, 1, 1, 3, 1, 0, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 1, 1, 1, 2, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 2, 3, 2, 3, 1, 3, 3, 3, 3, 3, 2, 1, 1, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 1,\n",
      "        1, 0, 3, 3, 1, 3, 0, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 2, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 2, 1, 3, 1, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 3, 2, 1, 3, 1, 1, 3, 3,\n",
      "        3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 2, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 1, 3, 0, 3, 0, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 3, 3, 3, 2, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 3, 0, 3,\n",
      "        1, 3, 3, 3, 1, 1, 3])\n",
      "> observation: tensor([-0.0915,  0.0498,  1.2192])\n",
      "tensor([2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 1, 2, 2, 2, 2, 2, 0, 0, 1, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2,\n",
      "        1, 2, 2, 2, 2, 2, 3, 0, 2, 2, 0, 3, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 1, 2, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 3,\n",
      "        2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 3, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2,\n",
      "        0, 2, 2, 2, 0, 0, 2])\n",
      "> observation: tensor([ 0.0702, -0.1929,  1.1425])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0265,  0.0686, -0.9648])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n",
      "        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.3666, -0.1578, -0.8046])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([0.0825, 0.1868, 0.9774])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0])\n",
      "> observation: tensor([ 0.1429, -0.2258, -0.9862])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.0121,  0.0357, -1.1670])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.1604, -0.0354,  1.0939])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.0634, -0.0360, -0.7851])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.0605, -0.0550,  1.0781])\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0470,  0.0397,  1.1046])\n",
      "tensor([2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.1255,  0.0792, -0.9115])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 0, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.3833,  0.0801, -1.1368])\n",
      "tensor([3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.0267, -0.1075,  0.8843])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1251, -0.0733,  0.9713])\n",
      "tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0467, -0.1077,  0.8241])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0671,  0.1109, -0.6380])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.0312, -0.0009,  0.8137])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0839, -0.0957, -0.8648])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 0, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.1854, -0.1437, -1.0346])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.1166,  0.0081, -0.9491])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.0792, -0.0295,  0.6878])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0807, -0.1412, -0.9534])\n",
      "tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.0041, -0.0600,  1.2400])\n",
      "tensor([2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2])\n",
      "> observation: tensor([ 0.0309, -0.0619,  0.9203])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1090, -0.1001,  0.6795])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1237, -0.0794,  1.2147])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1383, -0.0089, -1.1572])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([0.1913, 0.0899, 0.9692])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.0992, -0.0052,  1.1004])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1675,  0.0985, -0.8301])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 1, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.0031,  0.0067, -0.8337])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.0485,  0.3161,  1.0893])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([0.1301, 0.1046, 0.9387])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([0.1034, 0.2250, 0.8523])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.0626, -0.0123, -0.6077])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.1351, -0.0734, -0.7994])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 0, 3, 1, 3, 3, 3])\n",
      "> observation: tensor([0.0924, 0.1467, 1.0236])\n",
      "tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.1634,  0.3038,  1.1329])\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-0.9992,  0.1041,  0.0127])\n",
      "tensor([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 2, 3, 3, 1, 2, 1, 1, 1, 1, 1, 0, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 2, 3, 1,\n",
      "        1, 1, 1, 2, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 1, 1, 1, 1, 1])\n",
      "> observation: tensor([1.0841, 0.1922, 0.0352])\n",
      "tensor([2, 2, 2, 3, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 3,\n",
      "        2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 0, 2, 3, 0, 2, 2, 2, 2, 2, 0, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 0, 2, 3, 2, 3, 2, 2, 0, 2,\n",
      "        2, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 0, 0, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 3, 2, 2, 2, 3, 2, 0, 2, 2, 2, 2, 3, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 2, 3, 2, 3, 2, 2, 2, 3, 0, 2, 3, 2,\n",
      "        1, 0, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.6978, -0.3003,  0.1188])\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 1.0045, -0.0631, -0.0329])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2])\n",
      "> observation: tensor([ 1.0401,  0.0582, -0.2123])\n",
      "tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.1918, -0.1189, -0.0520])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3])\n",
      "> observation: tensor([-0.9768,  0.1449, -0.1143])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.8731, -0.0550, -0.2481])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
      "        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 1, 3, 3, 3, 3])\n",
      "> observation: tensor([-1.0990,  0.0602, -0.0069])\n",
      "tensor([3, 0, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 1.0775,  0.0570, -0.1176])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.9100, -0.2990, -0.1020])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.2727, -0.1604,  0.1399])\n",
      "tensor([3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 1])\n",
      "> observation: tensor([-0.7506, -0.0163, -0.0144])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.9510,  0.1092, -0.1477])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.9304, -0.0650, -0.0906])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.9219, -0.1322, -0.0465])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 1.0522, -0.0579, -0.0724])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.1830, -0.0470, -0.2361])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.8667,  0.1571,  0.0376])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 1.0289,  0.0801, -0.2469])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2])\n",
      "> observation: tensor([-0.8751,  0.1404,  0.0895])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([0.9657, 0.1508, 0.2291])\n",
      "tensor([0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.8145, -0.2249,  0.1326])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0])\n",
      "> observation: tensor([-0.9697,  0.1307, -0.1110])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 1])\n",
      "> observation: tensor([-0.8686, -0.0783,  0.1668])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.8469, -0.0683,  0.0301])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.8273, -0.0354, -0.0525])\n",
      "tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.1747,  0.0737,  0.0184])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 1, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.9346, -0.0392,  0.0079])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3])\n",
      "> observation: tensor([ 1.3658,  0.0555, -0.1342])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2])\n",
      "> observation: tensor([ 1.0017,  0.2271, -0.0442])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.1008,  0.0412, -0.2036])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.9099, -0.1431, -0.0555])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.8307, -0.2604,  0.0592])\n",
      "tensor([3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 3])\n",
      "> observation: tensor([ 1.0907,  0.0902, -0.2347])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 1.0528,  0.1495, -0.1235])\n",
      "tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([1.0574, 0.0422, 0.0159])\n",
      "tensor([2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 0.9440, -0.0475,  0.1340])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2])\n",
      "> observation: tensor([-1.1220, -0.1030,  0.1050])\n",
      "tensor([3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3])\n",
      "> observation: tensor([-1.1039,  0.0714,  0.0605])\n",
      "tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 1.1405, -0.0764,  0.0240])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([-1.0466,  0.2355, -0.2208])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.8982, -0.2465, -0.0331])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 1, 2, 2])\n",
      "> observation: tensor([-0.9486,  0.1015, -0.0409])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-1.0183, -0.3993, -0.1927])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-0.9368,  0.2381, -0.1260])\n",
      "tensor([3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([-1.0319,  0.2479, -0.2217])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3])\n",
      "> observation: tensor([ 0.9914, -0.1679,  0.0653])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 1.0525, -0.1241, -0.1286])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "> observation: tensor([ 1.2522,  0.2142, -0.1556])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Initialize ensemble\n",
    "ensemble = Ensemble(N=250, hyp_gamma=0.1, hyp_alpha_o=0.2, hyp_alpha_r=0.15)\n",
    "\n",
    "# Run 50 trials in context (c_o=0, c_r=0)\n",
    "batched_trials(ensemble, true_c_o_t=0, true_c_r_t=0, n_steps=50)\n",
    "# Run 50 trials in context (c_o=1, c_r=0)\n",
    "batched_trials(ensemble, true_c_o_t=1, true_c_r_t=0, n_steps=50)\n",
    "# Run 50 trials in context (c_o=2, c_r=1)\n",
    "batched_trials(ensemble, true_c_o_t=2, true_c_r_t=1, n_steps=50)\n",
    "# Run 50 trials in context (c_o=0, c_r=1)\n",
    "batched_trials(ensemble, true_c_o_t=0, true_c_r_t=1, n_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09285add",
   "metadata": {},
   "source": [
    "#### Sanity check of inferred parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9799bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4464387ad0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621df5f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a41580>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a41df0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4463344fb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a43080>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a42780>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208abd0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462088f50>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a41760>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a41010>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4926, 0.4926, 0.0148]) CRP counts: tensor([5., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462089df0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232bec0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208ac60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462328f80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623284a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208b080>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232ba70>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208b050>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a40c80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208b2f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232ad20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462089520>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462089c10>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627711f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4115, 0.5761, 0.0123]) CRP counts: tensor([5., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623294f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a0500>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232be60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208b200>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627734a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462329eb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a434a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462329ee0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a3530>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623288f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232a7e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232bfb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a39e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446208a930>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a2570>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a2180>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.7042, 0.1408, 0.1408, 0.0141]) CRP counts: tensor([10.,  2.,  2.])\n",
      "Reward Context CRP Probabilities: tensor([0.2827, 0.7067, 0.0106]) CRP counts: tensor([ 4., 10.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622706e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462273fe0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a1730>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232ba40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462328aa0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462273500>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622722a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627714c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462271ee0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462273200>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446232a600>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623a25a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462685d60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462942c60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446250fc20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446250eab0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621bf6b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462687380>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621de060>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462686930>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462da5fd0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621df8c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446265fb60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446265e9c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446265ea80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446265f560>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620eafc0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620e8320>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620e9370>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6863, 0.1961, 0.0980, 0.0196]) CRP counts: tensor([7., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620eaae0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620e9c40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446265cec0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622201d0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622228a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462220890>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622218e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462221be0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622231a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462223080>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462221ac0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462221d00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6135, 0.3681, 0.0184]) CRP counts: tensor([5., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44620e86b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462348fb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462348710>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446234be90>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462348f80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462539400>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446253b9e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462349a00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626ca780>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626c9490>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626ca360>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626cba10>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626c9430>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626cac30>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446327f830>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6818, 0.1515, 0.1515, 0.0152]) CRP counts: tensor([9., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3042, 0.6844, 0.0114]) CRP counts: tensor([4., 9.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446327fc80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44626c99a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621be4e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b3f80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b3290>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b3bf0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6818, 0.1515, 0.1515, 0.0152]) CRP counts: tensor([9., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3042, 0.6844, 0.0114]) CRP counts: tensor([4., 9.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b1430>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b3aa0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b0080>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b06e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b1d30>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b2ba0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b0c20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b33b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446253a570>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44624782c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446247be90>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462479040>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462e092e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622b3a70>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44624787a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446322cec0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625772f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462574980>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462576c00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462577350>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462577320>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462478500>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291d940>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291c4d0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291e090>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291f140>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291cf20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462ebfb00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a00800>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a022d0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a017c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a011f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a01070>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a019d0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a032c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a030b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a00890>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446291f740>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462128c20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212a090>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462128950>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462128260>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212b530>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212a720>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462129bb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462128440>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212a330>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212b8c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462a03d40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b8410>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b8710>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623ba810>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623bbd40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b80b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b8bf0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b81a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b8560>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623bbb30>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446212b4a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462648560>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462648290>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462649010>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623b90a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f64260>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f65e80>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f67710>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f66510>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f65850>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f66570>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f640b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f67a10>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f65190>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44621378c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462134fb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446264ad50>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a4ad0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a7ce0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a7b90>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a6ea0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a47a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4461f64e00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cdd30>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cd190>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cf4a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625ccb90>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cdd00>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cf470>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cd850>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cf080>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625ccfb0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627a4d70>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281e4e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281fc50>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281c290>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281c5f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4463f84950>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4463f87aa0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cf590>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446236e060>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.7042, 0.1408, 0.1408, 0.0141]) CRP counts: tensor([10.,  2.,  2.])\n",
      "Reward Context CRP Probabilities: tensor([0.2827, 0.7067, 0.0106]) CRP counts: tensor([ 4., 10.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281dee0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462874da0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628752b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6818, 0.1515, 0.1515, 0.0152]) CRP counts: tensor([9., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3042, 0.6844, 0.0114]) CRP counts: tensor([4., 9.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628764e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462876c60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462877230>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628764b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446236df70>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625bf290>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625bffe0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625bc7a0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625bc470>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628761b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6557, 0.1639, 0.1639, 0.0164]) CRP counts: tensor([8., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3292, 0.6584, 0.0123]) CRP counts: tensor([4., 8.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d11c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d3530>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d0bf0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d24b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6993, 0.2797, 0.0210]) CRP counts: tensor([5., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d3b60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4167, 0.2778, 0.2778, 0.0278]) CRP counts: tensor([3., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d1ee0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6818, 0.1515, 0.1515, 0.0152]) CRP counts: tensor([9., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3042, 0.6844, 0.0114]) CRP counts: tensor([4., 9.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44622d2390>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625be0f0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281b6e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462818110>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281ad20>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281ac60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462819be0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281bc50>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462819820>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c45765f64b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4463308a40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281b3e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4463308500>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628990d0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9935, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([46.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9942, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 51.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6098, 0.2439, 0.1220, 0.0244]) CRP counts: tensor([5., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6135, 0.3681, 0.0184]) CRP counts: tensor([5., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446289bf50>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4878, 0.2439, 0.2439, 0.0244]) CRP counts: tensor([4., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4908, 0.4908, 0.0184]) CRP counts: tensor([4., 4.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446289a3c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446289b110>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446289b980>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462898ce0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628985c0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6818, 0.1515, 0.1515, 0.0152]) CRP counts: tensor([9., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3042, 0.6844, 0.0114]) CRP counts: tensor([4., 9.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446289b6e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5882, 0.1961, 0.1961, 0.0196]) CRP counts: tensor([6., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3941, 0.5911, 0.0148]) CRP counts: tensor([4., 6.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44628989e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462898a40>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446281af60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.4839, 0.3226, 0.1613, 0.0323]) CRP counts: tensor([3., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.6504, 0.3252, 0.0244]) CRP counts: tensor([4., 2.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44627d92e0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c4462fb8a70>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6522, 0.2174, 0.1087, 0.0217]) CRP counts: tensor([6., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c446325c860>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.6250, 0.1786, 0.1786, 0.0179]) CRP counts: tensor([7., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.3587, 0.6278, 0.0135]) CRP counts: tensor([4., 7.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44623bb0b0>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5556, 0.2778, 0.1389, 0.0278]) CRP counts: tensor([4., 2., 1.])\n",
      "Reward Context CRP Probabilities: tensor([0.5594, 0.4196, 0.0210]) CRP counts: tensor([4., 3.])\n",
      "\n",
      ">> Particle <__main__.Particle object at 0x7c44625cdd60>:\n",
      "\n",
      "State 0, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([ 0.9979, -0.0597, -0.0257])\n",
      "State 0, Observ Context 1, Observation Model Params: df: 22.0, loc: tensor([-0.0380,  1.0444,  0.0175])\n",
      "State 0, Observ Context 2, Observation Model Params: df: 29.0, loc: tensor([-0.0278,  0.0265,  0.9891])\n",
      "State 1, Observ Context 0, Observation Model Params: df: 52.0, loc: tensor([-0.9850,  0.0097, -0.0472])\n",
      "State 1, Observ Context 1, Observation Model Params: df: 32.0, loc: tensor([-0.0535, -0.9924, -0.0081])\n",
      "State 1, Observ Context 2, Observation Model Params: df: 25.0, loc: tensor([-0.0205, -0.0063, -0.9508])\n",
      "\n",
      "State 0, Reward Context 0, Reward Model Alpha: tensor([0.9934, 0.0022, 0.0022, 0.0022]), Posterior Params: {'alpha_n': tensor([45.1000,  0.1000,  0.1000,  0.1000])}\n",
      "State 0, Reward Context 1, Reward Model Alpha: tensor([0.0019, 0.0019, 0.9943, 0.0019]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000, 52.1000,  0.1000])}\n",
      "State 1, Reward Context 0, Reward Model Alpha: tensor([0.0018, 0.9946, 0.0018, 0.0018]), Posterior Params: {'alpha_n': tensor([ 0.1000, 55.1000,  0.1000,  0.1000])}\n",
      "State 1, Reward Context 1, Reward Model Alpha: tensor([0.0021, 0.0021, 0.0021, 0.9938]), Posterior Params: {'alpha_n': tensor([ 0.1000,  0.1000,  0.1000, 48.1000])}\n",
      "\n",
      "Observation Context CRP Probabilities: tensor([0.5435, 0.2174, 0.2174, 0.0217]) CRP counts: tensor([5., 2., 2.])\n",
      "Reward Context CRP Probabilities: tensor([0.4372, 0.5464, 0.0164]) CRP counts: tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "for particle in ensemble.particles:\n",
    "    print(f\"\\n>> Particle {particle}:\\n\")\n",
    "    \n",
    "    for i, state in enumerate(particle.state_space):\n",
    "        for j, context in enumerate(particle.context_o_space):\n",
    "            print(f\"State {state}, Observ Context {context}, Observation Model Params: df: {particle.observation_models[i][j]._predictive_distribution_params()['df']}, loc: {particle.observation_models[i][j]._predictive_distribution_params()['loc']}\")\n",
    "    print()\n",
    "    for i, state in enumerate(particle.state_space):\n",
    "        for j, context in enumerate(particle.context_r_space):\n",
    "            print(f\"State {state}, Reward Context {context}, Reward Model Alpha: {particle.reward_models[i][j]._predictive_distribution_params()}, Posterior Params: {particle.reward_models[i][j].posterior_params()}\")\n",
    "\n",
    "    print()\n",
    "    # print the CRP probabilities\n",
    "    print(\"Observation Context CRP Probabilities:\", particle.context_o_CRP.probabilities, \"CRP counts:\", particle.context_o_CRP.counts)\n",
    "    print(\"Reward Context CRP Probabilities:\", particle.context_r_CRP.probabilities, \"CRP counts:\", particle.context_r_CRP.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421b5c0",
   "metadata": {},
   "source": [
    "### Visualisation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9eda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context O History:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAE8CAYAAACPRi2GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZO0lEQVR4nO3deVxUVf8H8M+dgRlQNhEBURS3FBRxSx4yl5TEJZOfWmglSErlo6aSZVSC1mOYlWnpo4+WmqVpmkulaYaiZrihhraYO6aCCyKKss2c3x/IlYkZZgYZBpjPu9d9ve6ce+be770dxvnOOfceSQghQERERERE9A8KawdARERERETVE5MFIiIiIiLSi8kCERERERHpxWSBiIiIiIj0YrJARERERER6MVkgIiIiIiK9mCwQEREREZFeTBaIiIiIiEgvJgtERERERKQXkwUiohps+vTpkCTJ2mHUaMnJyZAkCcnJydYOhYio2mGyQFSDnT59Gi+++CKaN28OBwcHuLi4oFu3bpg3bx7u3r1rseP+/vvvmD59Os6dO2exY5R49913sXHjRrPec/36dbz66qto3bo1HBwc4O7ujrCwMHz//fcm78PPzw+SJEGSJCgUCri5uSEwMBAvvPAC9u/fb+ZZVK2KXDNTpaen46WXXoKfnx/UajU8PT0RHh6OvXv3WuR4FTVq1Cj5/195y6hRo6wdKhFRtSYJIYS1gyAi823evBlPPfUU1Go1IiMj0a5dOxQUFODnn3/GN998g1GjRmHx4sUWOfa6devw1FNPYefOnejVq5dFjlHCyckJw4YNw/Lly02qf+LECfTp0wdXr15FdHQ0unTpguzsbKxcuRJHjx7FlClT8P777xvdj5+fH+rVq4dXXnkFAHDr1i388ccfWLt2LTIyMjB58mTMmTPnQU6tUhQVFaGoqAgODg5ymbnXzFR79+7FgAEDAABjxoxBQEAAMjIysHz5cpw+fRrz5s3DhAkTKvWYFZWSkoLTp0/Lr8+ePYv4+Hi88MIL6N69u1zeokULBAcHo6CgACqVCgoFf0MjItIhiKjGOXPmjHBychJt2rQRly5dKrP95MmTYu7cuRY7/tq1awUAsXPnTosdo0TdunVFVFSUSXULCgpEu3btRJ06dcS+fft0thUVFYmIiAgBQKxevdrovpo2bSoGDhxYpvzOnTsiPDxcABD//e9/TYqrqplzzUyVlZUlvL29hZeXlzh16pTOtjt37oju3bsLhUIh9u7dW6nHNebu3btCo9EYrXfw4EEBQCxbtszyQRER1SJMFohqoJdeekkAMPmLWWFhoXj77bdF8+bNhUqlEk2bNhVxcXEiLy9Pp17JF+Q9e/aIhx9+WKjVatGsWTPx+eefy3WWLVsmAJRZSicOW7ZsEY8++qioU6eOcHJyEgMGDBDHjx+XtyclJQlJksS0adN0jr9y5UqdL+H6jlPel+CvvvpKABBvv/223u3Z2dnCzc1NtGnTxug1M5QsCCHErVu3hLu7u2jUqJHQarVyuUajER999JEICAgQarVaeHp6ihdeeEFkZWXp3Xd511mI4uRn+vTpomXLlkKtVgt3d3fRrVs38eOPP8p1EhISROnffQxdsx07dggAYv369WXOp+S6//LLLwavR2JiogAgVqxYoXf7mTNnhFKpFGFhYUKI+1/Oly9fXqbu1q1bBQDx3XffyWV///23iI6OFp6enkKlUomAgADx2Wef6bxv586dAoD46quvxJtvvil8fHyEJEnixo0bBuMuUV6yULLf0m24Z8+eom3btuLXX38VPXr0EI6OjqJFixZi7dq1QgghkpOTRdeuXYWDg4N46KGHxPbt28vs15RzIiKq7pgsENVAjRo1Es2bNze5flRUlAAghg0bJhYsWCAiIyMFABEeHq5Tr2nTpqJ169bCy8tLvPHGG2L+/PmiU6dOQpIk+cv+6dOnxcsvvywAiDfeeEN88cUX4osvvhAZGRlCCCFWrFghJEkS/fr1E5988ol47733hJ+fn3BzcxNnz56VjzVu3DhhZ2cnUlNThRBCXLp0Sbi7u4vQ0FD5C/gXX3wh1Gq16N69u3yc8r7QPvPMMwKAOHfunNFrcfLkyXKvWXnJghBCjB49WgDQSYLGjBkj7OzsRExMjFi0aJGYOnWqqFu3rnj44YdFQUGBzr6NXWchhHjjjTeEJEkiJiZGLFmyRHz44YdixIgRYtasWXKdfyYLhq6ZVqsVvr6+YujQoWXOZcCAAaJFixblXo9HHnlEODg4lEkwS+vZs6ewt7cXd+7cEUII0bx5czFgwIAy9aKjo0W9evXka5KRkSEaN24sfH19xdtvvy0WLlwonnzySQFAfPTRR/L7Sr7UBwQEiA4dOog5c+aIxMREkZubW27sQlQsWfDx8RG+vr7i1VdfFZ988okICAgQSqVSrF69Wnh7e4vp06eLuXPnikaNGglXV1eRk5Mjv9/UcyIiqu6YLBDVMDdv3hQAxODBg02qf/ToUQFAjBkzRqd8ypQpAoDYsWOHXNa0aVMBQOzevVsuu3LlilCr1eKVV16RywwNQ7p165Zwc3MTMTExOuUZGRnC1dVVpzw3N1e0bNlStG3bVuTl5YmBAwcKFxcXcf78eZ33mjOkpkOHDsLV1bXcOnPmzBEAxLfffltuPWPJwkcffSQAiE2bNgkhhNizZ48AIFauXKlTr+RX9NLlpl7noKCgcmMQomyyIIThaxYXFyfUarXIzs7WOa6dnZ1ISEgo9zhubm4iKCio3DolSWRaWpp8PHt7e52elfz8fOHm5iaef/55uWz06NGiYcOG4tq1azr7Gz58uHB1dZWTj5Iv9c2bN5fLTFWRZAGAWLVqlVz2559/CgBCoVDoDHPbtm1bmX2bek5ERNUd7+QiqmFycnIAAM7OzibV37JlCwAgNjZWp7zkxt3NmzfrlAcEBOjcANqgQQO0bt0aZ86cMXqs7du3Izs7GyNGjMC1a9fkRalUIjg4GDt37pTr1qlTB8uXL8cff/yBHj16YPPmzfjoo4/QpEkTk85Ln1u3bhm9LiXbS65jRTk5OcnHBIC1a9fC1dUVjz/+uM65d+7cGU5OTjrnDph2nd3c3PDbb7/h5MmTDxRricjISOTn52PdunVy2Zo1a1BUVITnnnuu3PdW5NpGRESgsLAQ69evl+v8+OOPyM7ORkREBABACIFvvvkGgwYNghBC59qFhYXh5s2bOHz4sM5xoqKi4OjoaPqJV5CTkxOGDx8uv27dujXc3Nzg7++P4OBgubxkveT/XUXOiYiourKzdgBEZB4XFxcA97+kGnP+/HkoFAq0bNlSp9zb2xtubm44f/68Trm+L+v16tXDjRs3jB6r5Ett7969y429RLdu3TB27FgsWLAAYWFheP75540eozzOzs64du1auXVKrpupyZYht2/f1tnPyZMncfPmTXh6euqtf+XKFZ3Xplznt99+G4MHD8ZDDz2Edu3aoV+/fhg5ciTat29foZjbtGmDhx9+GCtXrsTo0aMBACtXrsS//vWvMu3jn5ydnY22uX9e26CgILRp0wZr1qyRj7dmzRp4eHjIbeTq1avIzs7G4sWLDT6965/XrlmzZkbOtHI0bty4zBwWrq6u8PX1LVMGQP5/V5FzIiKqrpgsENUwLi4u8PHxwfHjx816n6kTdymVSr3lwoSnLGu1WgDAF198AW9v7zLb7ex0P3Ly8/PlibBOnz6NO3fuoE6dOibFqY+/vz+OHj2K9PR0gz0UaWlpAIp/2X8QJde/5Eu2VquFp6cnVq5cqbd+gwYNdF6bcp179OiB06dPY9OmTfjxxx/x6aef4qOPPsKiRYswZsyYCsUdGRmJiRMn4u+//0Z+fj727duH+fPnG32fv78/jhw5gvz8fKjVar110tLSYG9vj1atWsllERERmDlzJq5duwZnZ2d8++23GDFihNwWStrMc889h6ioKL37/WdyVBW9CoDh/0fG/t9V5JyIiKorJgtENdATTzyBxYsXIyUlBSEhIeXWbdq0KbRaLU6ePAl/f3+5PDMzE9nZ2WjatKnZxzeUeLRo0QIA4OnpidDQUKP7SUhIwB9//IEPPvgAU6dOxeuvv46PP/7YpGPp88QTT+Crr77CihUr8NZbb5XZnpOTg02bNqFNmzZGf0kvz+3bt7Fhwwb4+vrK17RFixb46aef0K1bt0r9Muvu7o7o6GhER0fj9u3b6NGjB6ZPn15uslDeNRs+fDhiY2Px1Vdf4e7du7C3t5eHBJXniSeeQEpKCtauXat3yNK5c+ewZ88ehIaG6px/REQEZsyYgW+++QZeXl7IycnRGdrToEEDODs7Q6PRmNRmaoLaeE5EZLt4zwJRDfTaa6+hbt26GDNmDDIzM8tsL5kgC4A8idbcuXN16pRMKDZw4ECzj1+3bl0AQHZ2tk55WFgYXFxc8O6776KwsLDM+65evSqv79+/Hx988AEmTZqEV155Ba+++irmz5+PXbt2lTnWP49jyLBhwxAQEIBZs2bh0KFDOtu0Wi3Gjh2LGzduICEhwaT96XP37l2MHDkSWVlZePPNN+Uv5k8//TQ0Gg3eeeedMu8pKioy+RxKu379us5rJycntGzZEvn5+eW+r7xr5uHhgf79++PLL7/EypUr0a9fP3h4eBiN5cUXX4SnpydeffXVMvev5OXlITo6GkIIxMfH62zz9/dHYGAg1qxZgzVr1qBhw4bo0aOHvF2pVGLo0KH45ptv9PaWlW4zNUVtPCcisl3sWSCqgVq0aIFVq1YhIiIC/v7+OjM4//LLL1i7di1GjRoFoHjceFRUFBYvXozs7Gz07NkTBw4cwOeff47w8HA89thjZh+/Q4cOUCqVeO+993Dz5k2o1Wr07t0bnp6eWLhwIUaOHIlOnTph+PDhaNCgAdLT07F582Z069YN8+fPR15eHqKiotCqVSvMnDkTADBjxgx89913iI6OxrFjx+SEpHPnzvjpp58wZ84c+Pj4oFmzZjo3l5amUqmwbt069OnTB48++qjODM6rVq3C4cOH8corr+j8sl2eixcv4ssvvwRQ3Jvw+++/yzM4v/LKK3jxxRfluj179sSLL76IxMREHD16FH379oW9vT1OnjyJtWvXYt68eRg2bJhZ1zkgIAC9evVC586d4e7ujkOHDmHdunUYP358ue8zds0iIyPlWPQlN/rUr18f69atw8CBA9GpU6cyMzifOnUK8+bNwyOPPFLmvREREYiPj4eDgwNGjx5dZpbkWbNmYefOnQgODkZMTAwCAgKQlZWFw4cP46effkJWVpZJMVYntfGciMhGWe9BTET0oP766y8RExMj/Pz8hEqlEs7OzqJbt27ik08+0XkefmFhoZgxY4Zo1qyZsLe3F76+vuVOyvZPPXv2FD179tQpW7JkiWjevLlQKpVlHju5c+dOERYWJlxdXYWDg4No0aKFGDVqlDh06JAQQojJkycLpVIp9u/fr7PPQ4cOCTs7OzF27Fi57M8//5QnxYKRSdlKXLlyRcTGxsqTmbm5uYnQ0FCjj0v957XAvUnNJEkSLi4uom3btiImJqZM3KUtXrxYdO7cWTg6OgpnZ2cRGBgoXnvtNZ2Ztk29zv/5z39E165dhZubm3B0dBRt2rQRM2fO1JmzQd+jU41ds/z8fFGvXj3h6uoq7t69a/I1EUKIs2fPipiYGNGkSRNhb28vPDw8xJNPPin27Nlj8D0nT56Ur+XPP/+st05mZqYYN26c8PX1Ffb29sLb21v06dNHLF68WK5T8ojTkonRzFHRSdn+ydD/OwBi3LhxZp8TEVF1Jwlhwl2LRERUaxQVFcHHxweDBg3CZ599Zu1wiIioGuM9C0RENmbjxo24evUqIiMjrR0KERFVc+xZICKyEfv370daWhreeecdeHh4cGIwIiIyij0LREQ2YuHChRg7diw8PT2xYsUKa4dDREQ1AJMFIiIbsXz5chQVFeHQoUNo166dtcMhIqr1du/ejUGDBsHHxweSJGHjxo1G35OcnIxOnTpBrVajZcuWWL58ucXjLA+TBSIiIiIiC8jNzUVQUBAWLFhgUv2zZ89i4MCBeOyxx3D06FFMmjQJY8aMwbZt2ywcqWG8Z4GIiIiIyMIkScKGDRsQHh5usM7UqVOxefNmnQkdhw8fjuzsbGzdurUKoiyrRk/KptVqcenSJTg7O8uzqBIRERFR9SGEwK1bt+Dj41NmUsbqIC8vDwUFBSbVFUKU+c6pVquhVqsrJZaUlBSEhobqlIWFhWHSpEmVsv+KqNHJwqVLl+Dr62vtMIiIiIjIiAsXLqBx48bWDkNHXl4emjV1QsYVjUn1nZyccPv2bZ2yhIQETJ8+vVLiycjIgJeXl06Zl5cXcnJycPfuXTg6OlbKccxRo5MFZ2dnAMD5w35wcdLNVPNFobxeKO43AHtJKa/bQanzHqV0fx8aodVbXh3VpFhrsr7HB8nrvs7Z8np9Va68rhX3r/8vazrI6/Z3dUf7eSZfltev/6uhvK4p9cNEoVOpXy5KvV0q9Xlmn3t/Q6lDw/OndHm9oGkDnWPb/3auVEWP+7GfvyivK1o0uR/TX2fldTvf+7GisKhUUKViVd4PpCj9/j7Lo6hT6sNPe789a/Py7x/C7v7HlSgycOzKHFVpqf2aQOnmIq9r796/BiL//nrp+CTl/c8ynWtDZMS6E7/K6/z3gywl57YWTTudk7+3VScFBQXIuKLB2dSmcHEu/28g55YWzTqfx4ULF+Dicv9zurJ6FaqrGp0slHQDuTgpyvwPzi/1zamw1D/09qU+DO3+cX+3brKgv7w6qkmx1mQOKju8uHw3AGDLhEBo7Iu/oKnU97suSycLSrXD/XWN7pdNO8X9Dxal6n49qO6vatXGkwVlof5kofT+tXal9g/ATip1EGWpepK9vK4oVS6VKi+9XyhKJdulv1iX7mIu9d7yKErHJJVKFkqtS1KpZKH08XS6gy2ULFTmfk2ghhKj7qYCAJaiLYru/cghSl0PnWSh1I8ggkMyyQwuagmKWVkAACnOA1Cx/ZDlVOch445OAo5O5X/Wl3yfdHFx0UkWKpO3tzcyMzN1yjIzM+Hi4mKVXgWAT0MiMpmdRoNn1x3Es+sOwq5Ia/wNRBWkhBbD8o9hWP4x2FVxokI2pkhAuegmlItuAkVsa2S7tCb+Z2khISFISkrSKdu+fTtCQkIsfmxDmCwQERERkU3TCGHSYq7bt2/j6NGjOHr0KIDiR6MePXoU6enFw4Xj4uIQGRkp13/ppZdw5swZvPbaa/jzzz/x3//+F19//TUmT55cKedZETV6GBIRERER0YPSQkBrpCfX2HZ9Dh06hMcee0x+HRsbCwCIiorC8uXLcfnyZTlxAIBmzZph8+bNmDx5MubNm4fGjRvj008/RVhYmNnHrixMFoiIiIjIpmkhoLFAstCrVy+UN6WZvtmZe/XqhSNHjph9LEthskBERERENs1SPQu1AZMFIiIiIrJpptyTUJF7FmoDJgtEREREZNO09xZjdWwRkwUiE+Wr7PHs/54HAGjVfJAYWU4B7PCiyxAAQH6+0khtogfgIKFwZ/GMukqH6vsMfCJL05hwz4Kx7bUVkwUiEwmFhLN+xTMeN1HcsHI0VJsJScJ5Zb1763lWjoZqNYUEtL43KWI1njCLyNIKRfFirI4tYrJARERERDZNCwkalJ8wa41sr62YLBCZyK5Qg6jVKQCAHTFtoLHn8BCyDDuhwfC8XwEAq0RrFElsa2QhBQKKj+/1lE6sD6hs88sQkVYUL8bq2CImC0QmstNoMPrLXwAAu6MfYrJAFqOEFs/lFT9j+2v1QyiycjxUixUJKOdkAwC049yZLJDN0pjQs2Bse23FZIGIiIiIbBqTBcOYLBARERGRTdMKCVph5J4FI9trKyYLRERERGTT2LNgGJMFIiIiIrJpGiigQflzKGmqKJbqplrMLLVgwQL4+fnBwcEBwcHBOHDggLVDIiIiIiIbIe4NQypvETY6DMnqycKaNWsQGxuLhIQEHD58GEFBQQgLC8OVK1esHRoRERER2YACoTRpsUVWH4Y0Z84cxMTEIDo6GgCwaNEibN68GUuXLsXrr79u5eiI7iuwt8Poj0cWr6ts8wODqkYhlHjZ+UkAQEGB1X/TodpMLaFoiw8AQKG2zV9NiYDiCde0Rn5D18I2J1qwarJQUFCA1NRUxMXFyWUKhQKhoaFISUkpUz8/Px/5+fny65ycnCqJkwgAtEoF/mjdEADQRHnDytFQbaaVFPjLrkHxemGelaOhWk0pQXRwKF6XmCyQ7eINzoZZ9Sera9euQaPRwMvLS6fcy8sLGRkZZeonJibC1dVVXnx9fasqVCIiIiKqpTRCYdJii2rUWcfFxeHmzZvycuHCBWuHRDbErlCDZ9buxzNr90NZaKvPRKCqYCc0GJaXhmF5abATbGtkQQUCiv9mQ/HfbKDANodYEAElw5CML7bIqsOQPDw8oFQqkZmZqVOemZkJb2/vMvXVajXUanVVhUekw06jwfhPdwEADo5oBo0971sgy1BCizF3DwIAvlU3R5GV46FarEhA+Z8sAIA22g1Q2eaXISKtCY9OtdV7Fqzas6BSqdC5c2ckJSXJZVqtFklJSQgJCbFiZERERERkKzgMyTCrPw0pNjYWUVFR6NKlC7p27Yq5c+ciNzdXfjoSEREREZElaaHg05AMsHqyEBERgatXryI+Ph4ZGRno0KEDtm7dWuamZyIiIiIiS9AICRojk64Z215bWT1ZAIDx48dj/Pjx1g6DiIiIiGxQobBDoZFJ1wqZLBARERER2R6NCTc4azgMiYiIiIjI9mhhfJiRtmpCqXaYLBCZqMDeDuNmDy9eV/GxqWQ5hVDiNacBAICCQtt8+gZVEbWEonXFM9Mr1LY5xIIIMPUGZ9v8PGayQGQirVKBI0FNAABNlDesHA3VZlpJgTT74i9w2qI8K0dDtZpSgnjEsXhdYrJAtsuUR6Py0alERERERDbIlBmaOYMzEZVLWaTB4C2/AgCORDSFxs42f2Egy1MKLQbk/wkA+F74QSOxrZGFFAoovswpXh/pBtjb5pchIvYsGMZkgchE9kUaTFnwEwAgasgoJgtkMXbQYNzdFADANnUTo0/oIKqwQgHlm9cBANrhrkwWyGaZ9jQk2/wsZrJARERERDZNKyRojT0NifMsEBERERHZHq0JPQt8GhIRERERkQ0qFEoojc7gzEnZiIiIiIhsjlYooDVyA7Ox7bUVkwUiIiIismkaABojj0bVVE0o1Y5tpkhERERERPeU9CwYWypiwYIF8PPzg4ODA4KDg3HgwIFy68+dOxetW7eGo6MjfH19MXnyZOTlWW+CTvYsEJmo0N4OU94eem+9/HGNRA+iEErEOz0OACgo5G86ZEEqCUUrvAEACpVtPumFCLDcPAtr1qxBbGwsFi1ahODgYMydOxdhYWE4ceIEPD09y9RftWoVXn/9dSxduhSPPPII/vrrL4waNQqSJGHOnDlmH78yMFkgMpFGqcAvwS0AAE3sblg5GqrNtJICB+ybFK8XWe/XJLIBdhJEaJ3idYnJAtkuYcIMzqICMzjPmTMHMTExiI6OBgAsWrQImzdvxtKlS/H666+Xqf/LL7+gW7dueOaZZwAAfn5+GDFiBPbv32/2sSsLf7IiIiIiIptW0rNgbAGAnJwcnSU/P1/vPgsKCpCamorQ0FC5TKFQIDQ0FCkpKXrf88gjjyA1NVUeqnTmzBls2bIFAwYMqOQzNl2t6Fn4v4cCYSfZ65RJdqVOTVlqyIjm/u0pQvOPW1VKPxKr9C8s1f1RWTUp1hqsKNoD/U+lAgB2eHSERlHcri7d0d6vVOp/hWbMTXm9jqPur8NT4zbJ6zPOPCmv5xXdb7d3bte5v1vp/v9XpfL+8erWuXs/Pu393H/mWxvk9W232+kcO9L1iLz+a0F9eb2Hwy15/ce7v8vrA+vcP48D+fdPsK5UKK+XvimssNSj5/7lYNpwrRuaO/K6olR7dlU4yuv54v7x1KX+3jXi/vVQSpX3+4el9muKh2e8iAF/Fbe17T6d5Lbmsefi/fguZsjrorCgSuOj2mOQTwf0QToAIAlNoKnitk62oUgUAjhj7TDKZc6kbL6+vjrlCQkJmD59epn6165dg0ajgZeXl065l5cX/vzzT73HeOaZZ3Dt2jU8+uijEEKgqKgIL730Et544w0zzqZy1Ypkgagq2GmKMGP3GgDAnvD28hc4ospmry3COztWAwCSR7CtkeXYQ4tXcQgAsBuNjU5KRVRbaUyYlK1k+4ULF+Di4iKXq9XqSosjOTkZ7777Lv773/8iODgYp06dwsSJE/HOO+9g2rRplXYcczBZICIiIiKbVmTCpGxF93qcXVxcdJIFQzw8PKBUKpGZmalTnpmZCW9vb73vmTZtGkaOHIkxY8YAAAIDA5Gbm4sXXngBb775JhSKqk/o+RMCEREREdk0jZBMWsyhUqnQuXNnJCUlyWVarRZJSUkICQnR+547d+6USQiU94bTCysNNWfPAhERERHZNHPuWTBHbGwsoqKi0KVLF3Tt2hVz585Fbm6u/HSkyMhINGrUCImJiQCAQYMGYc6cOejYsaM8DGnatGkYNGiQnDRUNSYLRERERGTThAmTrokKzLMQERGBq1evIj4+HhkZGejQoQO2bt0q3/Scnp6u05Pw1ltvQZIkvPXWW7h48SIaNGiAQYMGYebMmWYfu7IwWSAiIiIim6aBpPNkP0N1KmL8+PEYP3683m3Jyck6r+3s7JCQkICEhIQKHcsSmCwQERERkU3TCuPDjLQ2+nR6JgtEJipU2uG13pHF63yUJVlQgdIOU8LutTUlP6bJcgqgwDv4l7xOZKu0JgxDMra9tuK/QkQm0iiU+Kl5EADAIVtrpDZRxWkUSmxv2QEAoLploz9lUZXQSgrsRmNrh0FkdVpI0BoZZmRse23FZIGIiIiIbJopj0Y199GptQWTBSITKbUaPHbuOAAgxS2As+qSxSi1GvQ+cwwA8HODdmxrZDEKocWjuAQA+Bk+0Eq2OcyCqEgoodAam5TNNj+LmSwQmcheU4TZO1YAAHqHz+QXOLIYlaYIH2wrbmvdR7zLtkYWo4IW07APADAI4cjjfQtko4QJw5AEhyEREREREdkeS03KVhswWSAiIiIim8anIRnGZIGIiIiIbBp7FgxjskBERERENo2PTjWMyQIRERER2TT2LBjGZIGIiIiIbBqTBcOsmizs3r0b77//PlJTU3H58mVs2LAB4eHh1gyJyKAipR0SekQAAAr5KEuyoEKFHab1Hl68ruRvOmQ5hVDgfXSR14lsFZMFw6z6r1Bubi6CgoLw/PPPY8iQIdYMhcioIoUS3z3UFQDgkK21cjRUmxUplfjWv7itqW4JK0dDtZlGUuBH+Fk7DCKr0wgJkpGnHXEGZyvo378/+vfvb80QiIiIiMjGsWfBMLOThatXr6JBgwZ6tx07dgyBgYEPHJQh+fn5yM/Pl1/n5ORY7FhE/6TUahDy9wkAwBGnVpxVlyxGqdXgkfTitnao3kNsa2QxCqFFF2QCAA7BC1qJQ5HINjFZMMzsT4XAwEBs3ry5TPkHH3yArl27VkpQhiQmJsLV1VVefH19LXo8otLsNUX4+MfP8PGPn8Feq7F2OFSLqTRFmL/5U8zf/CnsNUXWDodqMRW0mIm9mIm9UIHDK8l2lSQLxhZbZHayEBsbi6FDh2Ls2LG4e/cuLl68iD59+mD27NlYtWqVJWKUxcXF4ebNm/Jy4cIFix6PiIiIiGo/JguGmT0M6bXXXsPjjz+OkSNHon379sjKykJwcDDS0tLg7e1tiRhlarUaarXaoscgIiIiItsihARhJBkwtr22qtDgxJYtW6Jdu3Y4d+4ccnJyEBERYfFEgYiIiIjIEkpmcDa22CKzk4W9e/eiffv2OHnyJNLS0rBw4UJMmDABERERuHHjhln7un37No4ePYqjR48CAM6ePYujR48iPT3d3LCIiIiIiCqEw5AMMztZ6N27NyIiIrBv3z74+/tjzJgxOHLkCNLT081+EtKhQ4fQsWNHdOzYEUDx/RAdO3ZEfHy8uWEREREREVVIyTAkY4stMvuehR9//BE9e/bUKWvRogX27t2LmTNnmrWvXr16QQhOOERERERE1sNHpxpmdrJQkiicOnUKp0+fRo8ePeDo6AhJkjBt2rRKD5CouihS2mHWI/8HACjkc+/JggoVdni3R/Gs9oVKq86dSbVcIRT4BB3kdSJbpdUqoNGW/zegNbK9tjL7X6Hr16/j6aefxs6dOyFJEk6ePInmzZtj9OjRcHd3xwcffGCJOImsrkihxNcBjwIAHLL5PHKynCKlEmsCi9ua6hZ7X8lyNJIC36KltcMgsjoBwNhgF1v9NDY7RZo8eTLs7e2Rnp6OOnXqyOURERH44YcfKjU4IiIiIiJL49OQDKvQPQvbtm1D48aNdcpbtWqF8+fPV1pgRNWNQqtFx4wzAIA/HP2glWyzO5IsT6HVotPl4rZ23KkZtAq2NbIMhRBoh6sAgONoAK1km1+GiDjPgmFmJwu5ubk6PQolsrKyOGEa1WoqTSGWbFkIAOgdPhN5diorR0S1lVpTiM82/hcA0H3Eu8hT8LOVLEMFDT7EbgDAIIQjz/yvBUS1glZIkHiDs15m/1zVvXt3rFixQn4tSRK0Wi1mz56Nxx57rFKDIyIiIiKyNCFMW2yR2T8hzJ49G3369MGhQ4dQUFCA1157Db/99huysrKwd+9eS8RIRERERGQxHIZkmNk9C+3atcNff/2FRx99FIMHD0Zubi6GDBmCI0eOoEWLFpaIkYiIiIjIYjgpm2EVGpzo6uqKN998s7JjISIiIiKqcrxnwTCTkoW0tDSTd9i+ffsKB0NEREREVNW0WkDSGkkWbHSKJZOShQ4dOkCSJAghIJV6rJq4d6dH6TKNRlPJIRIRERERWQ7vWTDMpGTh7Nmz8vqRI0cwZcoUvPrqqwgJCQEApKSk4MMPP8Ts2bMtEyVRNVCkUGJu1yfurfO592Q5hQol5oSUtDWllaOh2qwICixGoLxOZKsEjM/QbKMPQzItWWjatKm8/tRTT+Hjjz/GgAED5LL27dvD19cX06ZNQ3h4eKUHSVQdFCntsKJ98eOBHbJttC+SqkSR0g6fd+oNAFDdstV/nqgqFEkKrEVra4dBZHWW7FlYsGAB3n//fWRkZCAoKAiffPIJunbtarB+dnY23nzzTaxfvx5ZWVlo2rQp5s6dq/PduyqZfYPzsWPH0KxZszLlzZo1w++//14pQRERERERVRkLdS2sWbMGsbGxWLRoEYKDgzF37lyEhYXhxIkT8PT0LFO/oKAAjz/+ODw9PbFu3To0atQI58+fh5ubm/kHryRm9zn6+/sjMTERBQUFcllBQQESExPh7+9fqcERVScKrRYBV9MRcDUdCsGeBbIchVaLtpnpaJuZDoWt3lFHVUIhBB4SWXhIZEFhqzNOEQGAKY9NrUDPwpw5cxATE4Po6GgEBARg0aJFqFOnDpYuXaq3/tKlS5GVlYWNGzeiW7du8PPzQ8+ePREUFPSgZ1hhZvcsLFq0CIMGDULjxo3lJx+lpaVBkiR89913lR4gUXWh0hTiy03zAAC9w2ciz05l5YiotlJrCrFq3VwAQPcR7yJPobZuQFRrqaDBAuwAAAxCOPIq9kR1ohrPlBmaS7bn5OTolKvVaqjVZT+nCwoKkJqairi4OLlMoVAgNDQUKSkpeo/x7bffIiQkBOPGjcOmTZvQoEEDPPPMM5g6dSqUSuvcw2b2p0LXrl1x5swZrFy5En/++ScAICIiAs888wzq1q1b6QFWlCgquv+i9LrJO6hBv7DUpFhrsPqfH5DX6248CKVU/p9Poy2Gt81EB3ndTnFRXnfS3n+amJOZ8ZX+mJqKYIP1duBRveUfGai/0Mw46MHlLs8FFhevq4dnQjgWJ6ZTZ3wr1wlS3ZXXHUq1RbVkb9IxNKV6x5QSb2y1VYN9Am33rk2iUsy5Z8HX11enPCEhAdOnTy9T/9q1a9BoNPDy8tIp9/Lykr9D/9OZM2ewY8cOPPvss9iyZQtOnTqFf//73ygsLERCQoJJ53LmzBk0b97cpLqmqNBPCHXr1sULL7xQaUEQEREREVmNKcOM7m2/cOECXFxc5GJ9vQoVpdVq4enpicWLF0OpVKJz5864ePEi3n//fZOThZYtW6Jnz54YPXo0hg0bBgcHhweKqULJwsmTJ7Fz505cuXIF2n+Mp42Pj3+ggIiIiIiIqpLQFi/G6gCAi4uLTrJgiIeHB5RKJTIzM3XKMzMz4e3trfc9DRs2hL29vc6QI39/f2RkZKCgoAAqlfEh0IcPH8ayZcsQGxuL8ePHIyIiAqNHjy73CUzlMbvvecmSJfD390d8fDzWrVuHDRs2yMvGjRsrFAQRERERkbUYu7nZlGFK/6RSqdC5c2ckJSXJZVqtFklJSfJcZf/UrVs3nDp1SufH+L/++gsNGzY0KVEAiidTnjdvHi5duoSlS5fi8uXLePTRR9GuXTvMmTMHV69eNes8zE4W/vOf/2DmzJnIyMjA0aNHceTIEXk5fPiwubsjIiIiIrI+YWSpgNjYWCxZsgSff/45/vjjD4wdOxa5ubmIjo4GAERGRurcAD127FhkZWVh4sSJ+Ouvv7B582a8++67GDdunNnHtrOzw5AhQ7B27Vq89957OHXqFKZMmQJfX19ERkbi8uXLpu3H3APfuHEDTz31lNkBExERERFVR5aalC0iIgJXr15FfHw8MjIy0KFDB2zdulW+6Tk9PR0Kxf3f7n19fbFt2zZMnjwZ7du3R6NGjTBx4kRMnTrV7GMfOnQIS5cuxerVq1G3bl1MmTIFo0ePxt9//40ZM2Zg8ODBOHDggNH9mJ0sPPXUU/jxxx/x0ksvmR00UU1WBAVWwF9eJ7KUIjsFPh7W6966dR6VR7ah+HMtQF4nslkWmpQNAMaPH4/x48fr3ZacnFymLCQkBPv27avYwVA8t8OyZctw4sQJDBgwACtWrMCAAQPkpKRZs2ZYvnw5/Pz8TNqf2clCy5YtMW3aNOzbtw+BgYGwt9d9TN/LL79s7i6JaoQiSYEv0NbaYZANKLSzw8dP9QYAeNvfsnI0VJsVSQp8Id37XONjuMmmSfcWY3Wqv4ULF+L555/HqFGj0LBhQ711PD098dlnn5m0P7OThcWLF8PJyQm7du3Crl27dLZJksRkgYiIiIhqFgv2LFS17du3o0mTJjrDmwBACIELFy6gSZMmUKlUiIqKMml/ZicLZ8+eNfctRLWCJASaoHjWxnS4QEg14xcGqnkkrRYtL14DANx2dYBQsK2RZeh8rglnfq6R7apFyUKLFi1w+fJleHp66pRnZWWhWbNm0Gg0Bt6pH+d1JzKRGhp8iu0AgEEIRx7/fMhCHAqK8MOU+QCA0E0Tkedo2uPyiMylhgafih8B8HONbJwZk7JVd8LAkMLbt29XaII2kz8VYmNjTao3Z84cs4MgIiIiIrIWcyZlq65KvqtLkoT4+HjUqVNH3qbRaLB//3506NDB7P2anCwcOXLEaB2J3ZdEREREVNPUgp6Fku/qQggcO3ZMZxI3lUqFoKAgTJkyxez9mpws7Ny50+ydExERERFVd5IoXozVqc5KvqtHR0dj3rx5cHFxqZT9cnAiEREREdm2WnSD87Jlyyp1f0wWiIiIiMi21fBhSEOGDMHy5cvh4uKCIUOGlFt3/fr1Zu2byQIRERER2bYa3rPg6uoq3zvs6upaqftmskBkoiIo8DUekteJLKXIToElg7rdW1daORqqzfi5RnRPDU8WSg89svowpPT0dPj6+pZ58lHpWeGIaqMiSYElaG/tMMgGFNrZ4b3nwgAA3va3rBwN1WZFkgJLpKDiFwaezU5kE2p4slDa3bt3IYSQH516/vx5bNiwAQEBAejbt6/Z+zP7Z4RmzZrh6tWrZcpLZoUzR2JiIh5++GE4OzvD09MT4eHhOHHihLkhERERERFVXMk9C8aWGmDw4MFYsWIFACA7Oxtdu3bFhx9+iMGDB2PhwoVm78/sZEEIoXc+hYrMCrdr1y6MGzcO+/btw/bt21FYWIi+ffsiNzfX3LCILE4SAl4iF14iFxJ/gSMLkrRaNLpyA42u3ICkZVsjy+HnGlGxkkenGltqgsOHD6N79+4AgHXr1sHb2xvnz5/HihUr8PHHH5u9P7NncJYkCdOmTauUWeG2bt2q83r58uXw9PREamoqevToYda+iCxNDQ2+xA8AgEEIRx5v+SELcSgowq4JHwEAQjdNRJ6jysg7iCpGDQ2+FFsA8HONbFwtGoZ0584dODs7AwB+/PFHDBkyBAqFAv/6179w/vx5s/dn9gzOlT0rXGk3b94EALi7u+vdnp+fj/z8fPl1Tk7OAx2PiIiIiEiCCZOyVUkkD65ly5bYuHEj/u///g/btm3D5MmTAQBXrlyp0ERtZs/gXNmzwpXQarWYNGkSunXrhnbt2umtk5iYiBkzZlTqcYmIiIjIxtXweRZKi4+PxzPPPIPJkyejT58+CAkJAVDcy9CxY0ez92f2PQuzZ882mCgcO3bM7ABKjBs3DsePH8fq1asN1omLi8PNmzfl5cKFCxU+HhERERERgPvDkIwtNcCwYcOQnp6OQ4cO6Qz579OnDz766COz92d2shAYGIjNmzeXKf/ggw/QtWtXswMAgPHjx+P777/Hzp070bhxY4P11Go1XFxcdBYiIiIiogdSi5IFAPD29kbHjh2hUNz/qt+1a1e0adPG7H2ZfSdTbGwshg4diujoaMyZMwdZWVmIjIzEsWPHsGrVKrP2JYTAhAkTsGHDBiQnJ5v96FUiIiIiogdlytOOasrTkHJzczFr1iwkJSXhypUr0Gq1OtvPnDlj1v7MThZee+01PP744xg5ciTat2+PrKwsBAcHIy0tDd7e3mbta9y4cVi1ahU2bdoEZ2dnZGRkACieptrR0dHc0IiIiIiIzFeLnoY0ZswY7Nq1CyNHjkTDhg31Tnlgjgo9I61ly5Zo164dvvnmGwBARESE2YkCAHliiF69eumUL1u2DKNGjapIaEQWo4GEb9FCXieyFI1SgS/7dpXXiSyFn2tE99SiZOGHH37A5s2b0a1bt0rZn9nJwt69e/Hcc8/B3d0daWlp2Lt3LyZMmIAtW7Zg0aJFqFevnsn7EpwAhmqQQkmJT2D+UwSIzFVgb4fpo58AAHirblk5GqrNCiUlPpE6Fb/gv8lkw2rTMKR69eoZnIagIsz+yap3796IiIjAvn374O/vjzFjxuDIkSNIT09HYGBgpQVGRERERFQltJJpSw3wzjvvID4+Hnfu3KmU/Znds/Djjz+iZ8+eOmUtWrTA3r17MXPmzEoJiqhaEgKuKAAA3IQKeMAxgEQGCQH3W/c+5N0E2xpZTunPNWHPtkY2qzb1LHz44Yc4ffo0vLy84OfnB3t7e53thw8fNmt/ZicLJYnCqVOncPr0afTo0QOOjo6QJAnTpk0zd3dENYYDNFiH7wAAgxCOvIrd8kNklGN+IQ7EvAcACN00EXmOKitHRLWVAzRYJ74FwM81snG16J6F8PDwSt2f2Z8K169fx9NPP42dO3dCkiScPHkSzZs3x+jRo+Hu7o4PPvigUgMkIiIiIrIoE3oWakqykJCQUKn7M/uehcmTJ8Pe3h7p6emoU6eOXB4REYEffvihUoMjIiIiIrK4WjYpW3Z2Nj799FPExcUhKysLQPHwo4sXL5q9rwrds7Bt27YyMy23atUK58+fNzsAIiIiIiKrqkXDkNLS0hAaGgpXV1ecO3cOMTExcHd3x/r165Geno4VK1aYtT+zexZyc3N1ehRKZGVlQa1Wm7s7IiIiIiKrKrnB2dhSE8TGxmLUqFE4efIkHBwc5PIBAwZg9+7dZu/P7GShe/fuOhmJJEnQarWYPXs2HnvsMbMDICIiIiKiynHw4EG8+OKLZcobNWqEjIwMs/dn9jCk2bNno0+fPjh06BAKCgrw2muv4bfffkNWVhb27t1rdgBERERERFZVi4YhqdVq5OTklCn/66+/0KBBA7P3Z3ay0K5dO/z111+YP38+nJ2dcfv2bQwZMgTjxo1Dw4YNzQ6AqKbQQMKPaCqvE1mKRqnANz07yOtElsLPNaJikgAkrfE6NcGTTz6Jt99+G19//TWA4lFA6enpmDp1KoYOHWr2/sxOFtLT0+Hr64s333xT77YmTZqYHQRRTVAoKfE+HrZ2GGQDCuztMPXfQwAA3qpbVo6GarNCSYn3pa7FL0QN+SZEZAm1qGfhww8/xLBhw9CgQQPcvXsXPXv2REZGBkJCQio0gbLZyUKzZs1w+fJleHp66pRfv34dzZo1g0ajMTsIIiIiIiJrqU0zOLu6umL79u3Yu3cvfv31V9y+fRudOnVCaGhohfZndv+2EAKSnungb9++rXPHNVGtIwQcRBEcRBF/gSPLEgKOeQVwzCtgWyPL4ucaUTELzrOwYMEC+Pn5wcHBAcHBwThw4IBJ71u9ejUkSTJrRmatVoulS5fiiSeewIsvvoiFCxfi559/xqVLlyAq+Dducs9CbGwsgOJxT9OmTdN5fKpGo8H+/fvRoUOHCgVBVBM4QIPvsBEAMAjhyDO/Y47IJI75hTgW9R8AQOimichzVFk5IqqtHKDBd2IDAH6ukW2zVM/CmjVrEBsbi0WLFiE4OBhz585FWFgYTpw4UWaUTmnnzp3DlClT0L17d5OPJYTAk08+iS1btiAoKAiBgYEQQuCPP/7AqFGjsH79emzcuNHsczD5U+HIkSNyIMeOHYNKdf8fL5VKhaCgIEyZMsXsAIiIiIiIrMpC9yzMmTMHMTExiI6OBgAsWrQImzdvxtKlS/H666/rfY9Go8Gzzz6LGTNmYM+ePcjOzjbpWMuXL8fu3buRlJRUZjqDHTt2IDw8HCtWrEBkZKRZ52BysrBz504AQHR0NObNmwcXFxezDkREREREVC2ZkSz887GkarVa78TEBQUFSE1NRVxcnFymUCgQGhqKlJQUg4d5++234enpidGjR2PPnj2mngG++uorvPHGG3rnPevduzdef/11rFy50uxkwex7FpYtW8ZEgYiIiIhqDXNmcPb19YWrq6u8JCYm6t3ntWvXoNFo4OXlpVPu5eVlcHK0n3/+GZ999hmWLFli9jmkpaWhX79+Brf3798fv/76q9n7NXtwYm5uLmbNmoWkpCRcuXIFWq3uQ2nPnDljdhBENk3LJ4iRLr+o4/K64+DzkKTij+qZUkf9b6jITWulH1TBG1ttloMosnYIRNWDGT0LFy5c0PnhXF+vQkXcunULI0eOxJIlS+Dh4WH2+7OyssokJqV5eXnhxo0bZu/X7GRhzJgx2LVrF0aOHImGDRvqfTISEREREVFNIWlNmJTt3nYXFxeTRtl4eHhAqVQiMzNTpzwzMxPe3t5l6p8+fRrnzp3DoEGD5LKSH+Xt7Oxw4sQJtGjRwuDxNBoN7OwMf7VXKpUoKjL/BwKzk4UffvgBmzdvRrdu3cw+GBERERFRtWOBG5xVKhU6d+6MpKQk+fGnWq0WSUlJGD9+fJn6bdq0wbFjx3TK3nrrLdy6dQvz5s2Dr69v+eEJgVGjRhns6cjPzzfvBO4xO1moV68e3N3dK3QwoppMAwm70UheJ7IUtjWqKmxrRMUs9ejU2NhYREVFoUuXLujatSvmzp2L3Nxc+elIkZGRaNSoERITE+Hg4IB27drpvN/NzQ0AypTrExUVZbSOuTc3AxVIFt555x3Ex8fj888/15lrgai2K5SUeAch1g6DbADbGlUVtjWieyz06NSIiAhcvXoV8fHxyMjIQIcOHbB161b53oL09HQoFGY/b0ivZcuWVcp+/kkSZk7n1rFjR5w+fRpCCPj5+cHe3l5n++HDhys1wPLk5OTA1dUVvTAYdpK98TcQEdVkhu4R4w3ORFSNFYlCJGMTbt68We2eqFnyXdL/3+9CqXYot64mPw9//PeNankelmR2z4I5U04TEREREVV30r3FWB1bZHaykJCQYIk4iKo9B1GE77ARADAI4ciTzP7zITIJ2xpVFbY1onssNAypNqjwp0Jqair++OMPAEDbtm3RsaOB538TEREREVVjlrrBuTYwO1m4cuUKhg8fjuTkZPkO7ezsbDz22GNYvXo1GjRoUNkxEhERERFZDnsWDDL79usJEybg1q1b+O2335CVlYWsrCwcP34cOTk5ePnlly0RIxERERGRZQkji40yu2dh69at+Omnn+Dv7y+XBQQEYMGCBejbt2+lBkdEREREZGnmzOBsa8xOFrRabZnHpQKAvb29PCU1EREREVFNwXsWDDN7GFLv3r0xceJEXLp0SS67ePEiJk+ejD59+lRqcEREREREFmdsCJIND0Uyu2dh/vz5ePLJJ+Hn5wdfX18AwIULF9CuXTt8+eWXlR4gUXWhgYT98JbXiSyFbY2qCtsaUTH2LBhmdrLg6+uLw4cP46effsKff/4JAPD390doaGilB0dUnRRKSryFR60dBtkAtjWqKmxrRPfwaUgGVWieBUmS8Pjjj+Pxxx+v7HiIiIiIiKoWkwWDTL5nYceOHQgICEBOTk6ZbTdv3kTbtm2xZ8+eSg2OiIiIiMjSSoYhGVtskcnJwty5cxETEwMXF5cy21xdXfHiiy9izpw5lRocUXXiIIrwrdiAb8UGOIgia4dDtRjbGlUVtjWie3iDs0EmJwu//vor+vXrZ3B73759kZqaatbBFy5ciPbt28PFxQUuLi4ICQnBDz/8YNY+iKqSIzRwhMbaYZANYFujqsK2RgRIQpi02CKTk4XMzEy98yuUsLOzw9WrV806eOPGjTFr1iykpqbi0KFD6N27NwYPHozffvvNrP0QEREREVVUyaRsxhZbZPINzo0aNcLx48fRsmVLvdvT0tLQsGFDsw4+aNAgndczZ87EwoULsW/fPrRt27ZM/fz8fOTn58uv9d0/QURERERkFt7gbJDJPQsDBgzAtGnTkJeXV2bb3bt3kZCQgCeeeKLCgWg0GqxevRq5ubkICQnRWycxMRGurq7yUjLPAxERERFRRfEGZ8NM7ll46623sH79ejz00EMYP348WrduDQD4888/sWDBAmg0Grz55ptmB3Ds2DGEhIQgLy8PTk5O2LBhAwICAvTWjYuLQ2xsrPw6JyeHCQMRERERPRj2LBhkcrLg5eWFX375BWPHjkVcXBzEvZs8JElCWFgYFixYAC8vL7MDaN26NY4ePYqbN29i3bp1iIqKwq5du/QmDGq1Gmq12uxjEBEREREZwhmcDTNrUramTZtiy5YtuHHjBk6dOgUhBFq1aoV69epVOACVSiXfB9G5c2ccPHgQ8+bNw//+978K75PIErSQ8Cs85HUiS2Fbo6rCtkZ0D3sWDKrQDM716tXDww8/XNmxAAC0Wq3OTcxE1UWBpMQU9LJ2GGQD2NaoqrCtEd1nqz0HxlQoWagscXFx6N+/P5o0aYJbt25h1apVSE5OxrZt26wZFhERERHZEiGKF2N1bJBVk4UrV64gMjISly9fhqurK9q3b49t27bh8ccft2ZYRERERGRDeM+CYVZNFj777DNrHp7ILA6iCF9gCwBgJAYgT7Lqnw/VYmxrVFXY1oiKSRpAMjKhgGSjE53zU4HIDG4osHYIZCPY1qiqsK0RgTc4l4PJAhERERHZNA5DMozJAhERERHZNt7gbBCTBSIiIiKyaexZMIzJAhERERHZNt6zYBCTBSIiIiKyaexZMIzJApGJtJBwAvXkdSJLYVujqsK2RnQP71kwiMkCkYkKJCXGo4+1wyAbwLZGVYVtjagYexYMY7JARERERDZN0hYvxurYIiYLRERERGTbtKJ4MVbHBhmZ2JqISqhFEb4QW/CF2AK1KLJ2OFSLsa1RVWFbI7pHmLjYIPYsEJlIAuCNO/I6kaWwrVFVYVsjKibBhHsWqiSS6oc9C0RERERk20qehmRsqYAFCxbAz88PDg4OCA4OxoEDBwzWXbJkCbp374569eqhXr16CA0NLbd+VWCyQEREREQ2reRpSMYWc61ZswaxsbFISEjA4cOHERQUhLCwMFy5ckVv/eTkZIwYMQI7d+5ESkoKfH190bdvX1y8ePEBz7DimCwQERERkW2z0D0Lc+bMQUxMDKKjoxEQEIBFixahTp06WLp0qd76K1euxL///W906NABbdq0waeffgqtVoukpKSKnVclYLJARERERDZNEsKkBQBycnJ0lvz8fL37LCgoQGpqKkJDQ+UyhUKB0NBQpKSkmBTXnTt3UFhYCHd39wc/yQpiskBEVFNU4hjaB34/EVFtojVxAeDr6wtXV1d5SUxM1LvLa9euQaPRwMvLS6fcy8sLGRkZJoU1depU+Pj46CQcVY1PQyIykQBwDi7yOpGlsK1RVWFbIypWuuegvDoAcOHCBbi4uMjlarXaIjHNmjULq1evRnJyMhwcHCxyDFMwWSAyUb5khxj0tXYYZAPY1qiqsK0R3WPGpGwuLi46yYIhHh4eUCqVyMzM1CnPzMyEt7d3ue/94IMPMGvWLPz0009o37690WNZEochEREREZFNs8TTkFQqFTp37qxzc3LJzcohISEG3zd79my888472Lp1K7p06VLRU6o07FkgIiIiIttmyj1cFbjHKzY2FlFRUejSpQu6du2KuXPnIjc3F9HR0QCAyMhINGrUSL7v4b333kN8fDxWrVoFPz8/+d4GJycnODk5mX38ysBkgchEalGE+dgBABiP3siX+OdDlsG2RlWFbY2omKQtXozVMVdERASuXr2K+Ph4ZGRkoEOHDti6dat803N6ejoUivsDfRYuXIiCggIMGzZMZz8JCQmYPn26+QFUAn4qEJlIAuCHHHmdyFLY1qiqsK0R3WOhngUAGD9+PMaPH693W3Jyss7rc+fOVegYlsRkgYiIiIhsmymTrtnoI8OYLBARERGRTTPn0am2hskCEREREdk2Cw5DqumYLBARERGRbROQZ2gut44NYrJARERERDZN0gpIRh53JBmbtK2WYrJAZCIBIAN15HUiS2Fbo6rCtkZ0D4chGcRkgchE+ZIdRmKAtcMgG8C2RlWFbY3oHi2MPz+4AvMs1AZMFoiIiIjIpvFpSIYxWSAiIiIi28ZhSAYxWSAykUpoMAfJAIBY9EKBpLRuQFRrsa1RVWFbI7qHyYJBTBaITKSAQGvckNeJLIVtjaoK2xrRPUwWDGKyQERERES2jTc4G6SwdgAlZs2aBUmSMGnSJGuHQkREREQ2pOQGZ2OLLaoWPQsHDx7E//73P7Rv397aoRARERGRrdFoYbTrQGObXQtW71m4ffs2nn32WSxZsgT16tWzdjhEREREZGtK7lkwttggqycL48aNw8CBAxEaGmq0bn5+PnJycnQWIiIiIqIHY0qiYJvJglWHIa1evRqHDx/GwYMHTaqfmJiIGTNmWDgqIsOyobJ2CGQj2NaoqrCtEYFPQyqH1ZKFCxcuYOLEidi+fTscHBxMek9cXBxiY2Pl1zk5OfD19bVUiEQ68iQ7PIUnrR0G2QC2NaoqbGtE92hN6DnQMlmoUqmpqbhy5Qo6deokl2k0GuzevRvz589Hfn4+lErdyWHUajXUanVVh0pEREREtZnQFi/G6tggqyULffr0wbFjx3TKoqOj0aZNG0ydOrVMokBEREREZBEchmSQ1ZIFZ2dntGvXTqesbt26qF+/fplyoupAJTR4F3sAAG+gOwokJrRkGWxrVFXY1oju4TAkg6rFPAtENYECAkG4Jq8TWQrbGlUVtjWie9izYFC1ShaSk5OtHQIRERER2RqtgNFJ2dizQERERERkg7QmzOCs5Q3ORERERES2h8OQDGKyQERERES2jcmCQUwWiIiIiMi28WlIBjFZIDLDXfCxglQ12NaoqrCtEQFCaCGMTLpmbHttxWSByER5kh2exP9ZOwyyAWxrVFXY1ojuEcJ4zwGHIRERERER2SBhwjAkJgtERERERDZIqwUkI8OMOAyJiMpjLzRIQAoAYAZCUChxnC9ZBtsaVRW2NaJ72LNgEJMFIhMpIRCMDHm90MrxUO3FtkZVhW2NqJjQaCAkTfl1RPnbaysmC0RERERk27QCkNizoA+TBSIiIiKybUIAMHbPgm0mCwprB0BEREREZE1CK0xaKmLBggXw8/ODg4MDgoODceDAgXLrr127Fm3atIGDgwMCAwOxZcuWCh23sjBZICIiIiLbJrSmLWZas2YNYmNjkZCQgMOHDyMoKAhhYWG4cuWK3vq//PILRowYgdGjR+PIkSMIDw9HeHg4jh8//qBnWGFMFoiIiIjIplmqZ2HOnDmIiYlBdHQ0AgICsGjRItSpUwdLly7VW3/evHno168fXn31Vfj7++Odd95Bp06dMH/+/Ac9xQqr0fcsiHtjx4pQaPRpV0QPqhBFyJHXC1Fko2MXyfLY1qiqsK1RVSi695wtUY3bV5HIN9pzUHIeOTk5OuVqtRpqtbpM/YKCAqSmpiIuLk4uUygUCA0NRUpKit5jpKSkIDY2VqcsLCwMGzduNOU0LKJGJwu3bt0CAPwM647lItvhKq9ttmIUZAvY1qiqsK1RVbl16xZcXV2NV6xCKpUK3t7e+DnDtO+STk5O8PX11SlLSEjA9OnTy9S9du0aNBoNvLy8dMq9vLzw559/6t1/RkaG3voZGRkmxWcJNTpZ8PHxwYULFyCEQJMmTXDhwgW4uLhYO6waIScnB76+vrxmZuJ1Mx+vWcXwupmP16xieN3Mx2tmHiEEbt26BR8fH2uHUoaDgwPOnj2LgoICk+oLISBJkk6Zvl6F2qRGJwsKhQKNGzeWu4NcXFz4R2smXrOK4XUzH69ZxfC6mY/XrGJ43czHa2a66tajUJqDgwMcHBwqfb8eHh5QKpXIzMzUKc/MzIS3t7fe93h7e5tVvyrwBmciIiIiokqmUqnQuXNnJCUlyWVarRZJSUkICQnR+56QkBCd+gCwfft2g/WrQo3uWSAiIiIiqq5iY2MRFRWFLl26oGvXrpg7dy5yc3MRHR0NAIiMjESjRo2QmJgIAJg4cSJ69uyJDz/8EAMHDsTq1atx6NAhLF682GrnUCuSBbVajYSEhFo/Zqwy8ZpVDK+b+XjNKobXzXy8ZhXD62Y+XjMyVUREBK5evYr4+HhkZGSgQ4cO2Lp1q3wTc3p6OhSK+wN9HnnkEaxatQpvvfUW3njjDbRq1QobN25Eu3btrHUKkER1fo4VERERERFZDe9ZICIiIiIivZgsEBERERGRXkwWiIiIiIhILyYLRERERESkV41PFhYsWAA/Pz84ODggODgYBw4csHZI1UZiYiIefvhhODs7w9PTE+Hh4Thx4oROnV69ekGSJJ3lpZdeslLE1cP06dPLXJM2bdrI2/Py8jBu3DjUr18fTk5OGDp0aJkJVGyRn59fmesmSRLGjRsHgG0NAHbv3o1BgwbBx8cHkiRh48aNOtuFEIiPj0fDhg3h6OiI0NBQnDx5UqdOVlYWnn32Wbi4uMDNzQ2jR4/G7du3q/Asql55162wsBBTp05FYGAg6tatCx8fH0RGRuLSpUs6+9DXPmfNmlXFZ1J1jLW1UaNGlbke/fr106nDtlb2uun7jJMkCe+//75cx9baGtV+NTpZWLNmDWJjY5GQkIDDhw8jKCgIYWFhuHLlirVDqxZ27dqFcePGYd++fdi+fTsKCwvRt29f5Obm6tSLiYnB5cuX5WX27NlWirj6aNu2rc41+fnnn+VtkydPxnfffYe1a9di165duHTpEoYMGWLFaKuHgwcP6lyz7du3AwCeeuopuY6tt7Xc3FwEBQVhwYIFerfPnj0bH3/8MRYtWoT9+/ejbt26CAsLQ15enlzn2WefxW+//Ybt27fj+++/x+7du/HCCy9U1SlYRXnX7c6dOzh8+DCmTZuGw4cPY/369Thx4gSefPLJMnXffvttnfY3YcKEqgjfKoy1NQDo16+fzvX46quvdLazrZVV+npdvnwZS5cuhSRJGDp0qE49W2prZANEDda1a1cxbtw4+bVGoxE+Pj4iMTHRilFVX1euXBEAxK5du+Synj17iokTJ1ovqGooISFBBAUF6d2WnZ0t7O3txdq1a+WyP/74QwAQKSkpVRRhzTBx4kTRokULodVqhRBsa/8EQGzYsEF+rdVqhbe3t3j//fflsuzsbKFWq8VXX30lhBDi999/FwDEwYMH5To//PCDkCRJXLx4scpit6Z/Xjd9Dhw4IACI8+fPy2VNmzYVH330kWWDq6b0XbOoqCgxePBgg+9hWzOtrQ0ePFj07t1bp8yW2xrVTjW2Z6GgoACpqakIDQ2VyxQKBUJDQ5GSkmLFyKqvmzdvAgDc3d11yleuXAkPDw+0a9cOcXFxuHPnjjXCq1ZOnjwJHx8fNG/eHM8++yzS09MBAKmpqSgsLNRpd23atEGTJk3Y7kopKCjAl19+ieeffx6SJMnlbGuGnT17FhkZGTpty9XVFcHBwXLbSklJgZubG7p06SLXCQ0NhUKhwP79+6s85urq5s2bkCQJbm5uOuWzZs1C/fr10bFjR7z//vsoKiqyToDVRHJyMjw9PdG6dWuMHTsW169fl7exrRmXmZmJzZs3Y/To0WW2sa1RbVJjZ3C+du0aNBqNPANeCS8vL/z5559Wiqr60mq1mDRpErp166YzC+AzzzyDpk2bwsfHB2lpaZg6dSpOnDiB9evXWzFa6woODsby5cvRunVrXL58GTNmzED37t1x/PhxZGRkQKVSlfkS4uXlhYyMDOsEXA1t3LgR2dnZGDVqlFzGtla+kvaj7zOtZFtGRgY8PT11ttvZ2cHd3Z3t7568vDxMnToVI0aMgIuLi1z+8ssvo1OnTnB3d8cvv/yCuLg4XL58GXPmzLFitNbTr18/DBkyBM2aNcPp06fxxhtvoH///khJSYFSqWRbM8Hnn38OZ2fnMsNQ2daotqmxyQKZZ9y4cTh+/LjO2HsAOuNPAwMD0bBhQ/Tp0wenT59GixYtqjrMaqF///7yevv27REcHIymTZvi66+/hqOjoxUjqzk+++wz9O/fHz4+PnIZ2xpZWmFhIZ5++mkIIbBw4UKdbbGxsfJ6+/btoVKp8OKLLyIxMRFqtbqqQ7W64cOHy+uBgYFo3749WrRogeTkZPTp08eKkdUcS5cuxbPPPgsHBwedcrY1qm1q7DAkDw8PKJXKMk+hyczMhLe3t5Wiqp7Gjx+P77//Hjt37kTjxo3LrRscHAwAOHXqVFWEViO4ubnhoYcewqlTp+Dt7Y2CggJkZ2fr1GG7u+/8+fP46aefMGbMmHLrsa3pKmk/5X2meXt7l3mAQ1FREbKysmy+/ZUkCufPn8f27dt1ehX0CQ4ORlFREc6dO1c1AVZzzZs3h4eHh/z3yLZWvj179uDEiRNGP+cAtjWq+WpssqBSqdC5c2ckJSXJZVqtFklJSQgJCbFiZNWHEALjx4/Hhg0bsGPHDjRr1szoe44ePQoAaNiwoYWjqzlu376N06dPo2HDhujcuTPs7e112t2JEyeQnp7OdnfPsmXL4OnpiYEDB5Zbj21NV7NmzeDt7a3TtnJycrB//365bYWEhCA7OxupqalynR07dkCr1crJly0qSRROnjyJn376CfXr1zf6nqNHj0KhUJQZamOr/v77b1y/fl3+e2RbK99nn32Gzp07IygoyGhdtjWq6Wr0MKTY2FhERUWhS5cu6Nq1K+bOnYvc3FxER0dbO7RqYdy4cVi1ahU2bdoEZ2dneZypq6srHB0dcfr0aaxatQoDBgxA/fr1kZaWhsmTJ6NHjx5o3769laO3nilTpmDQoEFo2rQpLl26hISEBCiVSowYMQKurq4YPXo0YmNj4e7uDhcXF0yYMAEhISH417/+Ze3QrU6r1WLZsmWIioqCnd39jxe2tWK3b9/W6Uk5e/Ysjh49Cnd3dzRp0gSTJk3Cf/7zH7Rq1QrNmjXDtGnT4OPjg/DwcACAv78/+vXrh5iYGCxatAiFhYUYP348hg8frjPkq7Yp77o1bNgQw4YNw+HDh/H9999Do9HIn3Xu7u5QqVRISUnB/v378dhjj8HZ2RkpKSmYPHkynnvuOdSrV89ap2VR5V0zd3d3zJgxA0OHDoW3tzdOnz6N1157DS1btkRYWBgAtrUS//wbBYqT+LVr1+LDDz8s835bbGtkA6z9OKYH9cknn4gmTZoIlUolunbtKvbt22ftkKoNAHqXZcuWCSGESE9PFz169BDu7u5CrVaLli1bildffVXcvHnTuoFbWUREhGjYsKFQqVSiUaNGIiIiQpw6dUrefvfuXfHvf/9b1KtXT9SpU0f83//9n7h8+bIVI64+tm3bJgCIEydO6JSzrRXbuXOn3r/JqKgoIUTx41OnTZsmvLy8hFqtFn369ClzLa9fvy5GjBghnJychIuLi4iOjha3bt2ywtlUnfKu29mzZw1+1u3cuVMIIURqaqoIDg4Wrq6uwsHBQfj7+4t3331X5OXlWffELKi8a3bnzh3Rt29f0aBBA2Fvby+aNm0qYmJiREZGhs4+2NbK/o0KIcT//vc/4ejoKLKzs8u83xbbGtV+khBCWDwjISIiIiKiGqfG3rNARERERESWxWSBiIiIiIj0YrJARERERER6MVkgIiIiIiK9mCwQEREREZFeTBaIiIiIiEgvJgtERERERKQXkwUiIiIiItKLyQIRkYlGjRqF8PBwa4dBRERUZZgsEBEBkCSp3GX69OmYN28eli9fbpX4lixZgqCgIDg5OcHNzQ0dO3ZEYmKivJ2JDBERWYKdtQMgIqoOLl++LK+vWbMG8fHxOHHihFzm5OQEJycna4SGpUuXYtKkSfj444/Rs2dP5OfnIy0tDcePH7dKPEREZDvYs0BEBMDb21teXF1dIUmSTpmTk1OZX+979eqFCRMmYNKkSahXrx68vLywZMkS5ObmIjo6Gs7OzmjZsiV++OEHnWMdP34c/fv3h5OTE7y8vDBy5Ehcu3bNYGzffvstnn76aYwePRotW7ZE27ZtMWLECMycORMAMH36dHz++efYtGmT3BOSnJwMALhw4QKefvppuLm5wd3dHYMHD8a5c+fkfZec04wZM9CgQQO4uLjgpZdeQkFBQaVdWyIiqrmYLBARPYDPP/8cHh4eOHDgACZMmICxY8fiqaeewiOPPILDhw+jb9++GDlyJO7cuQMAyM7ORu/evdGxY0ccOnQIW7duRWZmJp5++mmDx/D29sa+fftw/vx5vdunTJmCp59+Gv369cPly5dx+fJlPPLIIygsLERYWBicnZ2xZ88e7N27F05OTujXr59OMpCUlIQ//vgDycnJ+Oqrr7B+/XrMmDGjci8UERHVSEwWiIgeQFBQEN566y20atUKcXFxcHBwgIeHB2JiYtCqVSvEx8fj+vXrSEtLAwDMnz8fHTt2xLvvvos2bdqgY8eOWLp0KXbu3Im//vpL7zESEhLg5uYGPz8/tG7dGqNGjcLXX38NrVYLoHiIlKOjI9RqtdwTolKpsGbNGmi1Wnz66acIDAyEv78/li1bhvT0dLnnAQBUKhWWLl2Ktm3bYuDAgXj77bfx8ccfy/snIiLbxWSBiOgBtG/fXl5XKpWoX78+AgMD5TIvLy8AwJUrVwAAv/76K3bu3CnfA+Hk5IQ2bdoAAE6fPq33GA0bNkRKSgqOHTuGiRMnoqioCFFRUejXr1+5X+h//fVXnDp1Cs7OzvKx3N3dkZeXp3OsoKAg1KlTR34dEhKC27dv48KFCxW4IkREVJvwBmciogdgb2+v81qSJJ0ySZIAQP5Sf/v2bQwaNAjvvfdemX01bNiw3GO1a9cO7dq1w7///W+89NJL6N69O3bt2oXHHntMb/3bt2+jc+fOWLlyZZltDRo0KP/EiIiIwGSBiKhKderUCd988w38/PxgZ1fxj+CAgAAAQG5uLoDioUQajabMsdasWQNPT0+4uLgY3Nevv/6Ku3fvwtHREQCwb98+ODk5wdfXt8LxERFR7cBhSEREVWjcuHHIysrCiBEjcPDgQZw+fRrbtm1DdHR0mS/7JcaOHYt33nkHe/fuxfnz57Fv3z5ERkaiQYMGCAkJAQD4+fkhLS0NJ06cwLVr11BYWIhnn30WHh4eGDx4MPbs2YOzZ88iOTkZL7/8Mv7++295/wUFBRg9ejR+//13bNmyBQkJCRg/fjwUCv4TQURk6/gvARFRFfLx8cHevXuh0WjQt29fBAYGYtKkSXBzczP45Tw0NBT79u3DU089hYceeghDhw6Fg4MDkpKSUL9+fQBATEwMWrdujS5duqBBgwbYu3cv6tSpg927d6NJkyYYMmQI/P39MXr0aOTl5en0NPTp0wetWrVCjx49EBERgSeffBLTp0+vistBRETVnCSEENYOgoiIrGPUqFHIzs7Gxo0brR0KERFVQ+xZICIiIiIivZgsEBERERGRXhyGREREREREerFngYiIiIiI9GKyQEREREREejFZICIiIiIivZgsEBERERGRXkwWiIiIiIhILyYLRERERESkF5MFIiIiIiLSi8kCERERERHp9f+gTZFu9YopsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context R History:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions History:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Optimal Actions Over Time:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump History:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3987996/1324857792.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jumps_densities = torch.tensor(torch.stack(jumps_hist), dtype=torch.float).sum(dim=1) / ensemble.N\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Context O History:\")\n",
    "context_o_densities = torch.zeros((len(context_o_hist), 5))\n",
    "for idx, element in enumerate(context_o_hist):\n",
    "    #print(element)\n",
    "    for val in element:\n",
    "        context_o_densities[idx, val] += 1\n",
    "context_o_densities = context_o_densities / context_o_densities.sum(dim=1, keepdim=True)\n",
    "\n",
    "# set size of plot\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(context_o_densities.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Density')\n",
    "plt.axvline(x=50, color='red', linestyle='--', label='Obs Context Change')\n",
    "plt.axvline(x=100, color='red', linestyle='--')\n",
    "plt.axvline(x=150, color='red', linestyle='--')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Context Index')\n",
    "plt.title('Context O Density Over Time')\n",
    "plt.show()\n",
    "\n",
    "print(\"Context R History:\")\n",
    "context_r_densities = torch.zeros((len(context_r_hist), 6))\n",
    "for idx, element in enumerate(context_r_hist):\n",
    "    #print(element)\n",
    "    for val in element:\n",
    "        context_r_densities[idx, val] += 1\n",
    "context_r_densities = context_r_densities / context_r_densities.sum(dim=1, keepdim=True)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(context_r_densities.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Density')\n",
    "plt.axvline(x=100, color='red', linestyle='--', label='Action Context Change')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Context Index')\n",
    "plt.title('Context R Density Over Time')\n",
    "plt.show()\n",
    "\n",
    "print(\"Actions History:\")\n",
    "actions_densities = torch.zeros((len(actions_hist), 4))\n",
    "for idx, element in enumerate(actions_hist):\n",
    "    #print(element)\n",
    "    for val in element:\n",
    "        actions_densities[idx, val] += 1\n",
    "actions_densities = actions_densities / actions_densities.sum(dim=1, keepdim=True)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(actions_densities.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Density')\n",
    "plt.yticks(ticks=[0, 1, 2, 3], labels=['A0', 'A1', 'A2', 'A3'])\n",
    "plt.axvline(x=50, color='red', linestyle='--')\n",
    "plt.axvline(x=100, color='red', linestyle='--')\n",
    "plt.axvline(x=150, color='red', linestyle='--')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Action Index')\n",
    "plt.title('Action Density Over Time')\n",
    "plt.show()\n",
    "\n",
    "print(\"Proportion of Optimal Actions Over Time:\")\n",
    "true_rewarding_actions = torch.tensor(true_rewarding_actions)\n",
    "proportion_optimal = torch.zeros(len(actions_hist))\n",
    "for idx, element in enumerate(actions_hist):\n",
    "    proportion_optimal[idx] = torch.sum(element == true_rewarding_actions[idx]).item() / len(element)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(proportion_optimal, label='Proportion Optimal Action', color='blue', linewidth=2)\n",
    "plt.title('Proportion of Optimal Actions Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Proportion')\n",
    "plt.ylim(0, 1)\n",
    "plt.axvline(x=50, color='red', linestyle='--')\n",
    "plt.axvline(x=100, color='red', linestyle='--')\n",
    "plt.axvline(x=150, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(\"Jump History:\")\n",
    "jumps_densities = torch.tensor(torch.stack(jumps_hist), dtype=torch.float).sum(dim=1) / ensemble.N\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(jumps_densities, label='Jump', color='blue', linewidth=2)\n",
    "plt.title('Jump Density Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Density')\n",
    "plt.ylim(0, 1)\n",
    "plt.axvline(x=50, color='red', linestyle='--')\n",
    "plt.axvline(x=100, color='red', linestyle='--')\n",
    "plt.axvline(x=150, color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic312 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
